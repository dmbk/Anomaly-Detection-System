{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST_AAE_ROC_test_PED1_PED2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbk/Anomaly-Detection-System/blob/master/TEST_AAE_ROC_test_PED1_PED2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVE526Rfl8ed",
        "colab_type": "code",
        "outputId": "ea6dc311-7b0d-4c1d-d74f-24fa4c3603ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "!pip install imageio\n",
        "!pip install qpsolvers\n",
        "!pip install shapely \n",
        "#!pip install tensorflow_datasets\n",
        "!pip install keras-layer-normalization\n",
        "from google.colab import drive\n",
        "#!pip install alive-progress\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.4)\n",
            "Collecting qpsolvers\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/10/b9a34d08edd4f492498dcb46c016cc02866a73132efb197c9d2865e21672/qpsolvers-1.1.tar.gz\n",
            "Collecting quadprog\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/f0/d4c8866f5d14dfa1a441439f5ce0d2680844651772129c431e78caadfe10/quadprog-0.1.7.tar.gz\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from quadprog->qpsolvers) (0.29.17)\n",
            "Building wheels for collected packages: qpsolvers, quadprog\n",
            "  Building wheel for qpsolvers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qpsolvers: filename=qpsolvers-1.1-cp36-none-any.whl size=16348 sha256=079c18b3c83eed228b103d86e4549e02f58535dbcb5d645fa2e915bf6e5ef80c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/4b/c9/a05d1e02e4a031c89d46a3f5a68dadc6280999680ed01c6084\n",
            "  Building wheel for quadprog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quadprog: filename=quadprog-0.1.7-cp36-cp36m-linux_x86_64.whl size=307621 sha256=61f69f39853d99590b67ed5d63d0c75d17df1eb17d0e7ed49ce7bb1cf208c92d\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/dd/b1/849989444c0a5930927b260663019b7da6cff864fc224c2747\n",
            "Successfully built qpsolvers quadprog\n",
            "Installing collected packages: quadprog, qpsolvers\n",
            "Successfully installed qpsolvers-1.1 quadprog-0.1.7\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Collecting keras-layer-normalization\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (1.18.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.4.1)\n",
            "Building wheels for collected packages: keras-layer-normalization\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=eb09ea2e516a14949ce2e637e48c9d24b95f9037e44240066031e2c31a3db40b\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "Successfully built keras-layer-normalization\n",
            "Installing collected packages: keras-layer-normalization\n",
            "Successfully installed keras-layer-normalization-0.14.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drSah6JgmAU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57c3b272-c779-4572-8023-49ad7953c72a"
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import skimage\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "#import keras\n",
        "import argparse\n",
        "from os.path import dirname\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\t\n",
        "import statistics\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "#from progress.bar import IncrementalBar\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D, Dropout, Activation, InputLayer\n",
        "from keras.optimizers import Adam\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJ798qPmI9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_single_test(single_test_path):\n",
        "    \n",
        "    sz = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "          sz = sz +1\n",
        "    test = np.zeros(shape=(sz, conf.dim2, conf.dim3, conf.dim4))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "            #print(\"img path: \"+join(single_test_path, f))\n",
        "            img = Image.open(join(single_test_path, f)).resize((conf.dim2, conf.dim3))\n",
        "            #cv2_imshow(np.array(img,dtype=np.float32))\n",
        "            #cv2.waitKey(0)\n",
        "            img = np.array(img, dtype=np.float32) / 256\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    return test\n",
        "\n",
        "def get_test_sequences(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    print(\"Test case loaded\")\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "\n",
        "def prepend_10_clips(test_cases):\n",
        "    test_aug = []\n",
        "    test_aug.extend(test_cases[0:10])\n",
        "    test_aug.extend(test_cases)\n",
        "    return test_aug\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgtzofEvwlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    def __init__(self, data_dir_, cwdir_name_, data_set):\n",
        "        self.data_set_name = data_set\n",
        "        self.data_dir = data_dir_\n",
        "        self.data_set_dir = join(self.data_dir, data_set)\n",
        "        self.cwdir_name = cwdir_name_\n",
        "        self.cwdir = join(self.data_dir,self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "        if not os.path.exists(self.cwdir):\n",
        "            os.mkdir(self.cwdir)\n",
        "            os.mkdir(self.run_data)\n",
        "    \n",
        "        if not os.path.exists(self.run_data):\n",
        "            #shutil.rmtree(self.run_data)\n",
        "            os.mkdir(self.run_data)\n",
        "            os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "        self.DATASET_PATH = join(self.data_set_dir,\"Train/\")\n",
        "        self.TEST_DIR = join(self.data_set_dir,\"Test/\")\n",
        "        self.BATCH_SIZE = 2\n",
        "        self.EPOCHS = 50\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = 0\n",
        "        self.dim1 = 10\n",
        "        self.dim2 = 256\n",
        "        self.dim3 = 256\n",
        "        self.dim4 = 1\n",
        "        self.latent_dim = 327680\n",
        "\n",
        "\n",
        "    def reconfig(self, new_name, batch_size = 4, epochs = 5, retrain = 0):\n",
        "        self.cwdir_name = new_name\n",
        "        self.cwdir = join(self.data_dir, self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.EPOCHS = epochs\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = retrain\n",
        "        if retrain == 0:\n",
        "            print(\"Configuring train from scratch\")\n",
        "            if not os.path.exists(self.cwdir):\n",
        "                os.mkdir(self.cwdir)\n",
        "                os.mkdir(self.run_data)\n",
        "    \n",
        "            if os.path.exists(self.run_data):\n",
        "                shutil.rmtree(self.run_data)\n",
        "                os.mkdir(self.run_data)\n",
        "                os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "conf = Config(data_dir_=\"/content/drive/My Drive/\", cwdir_name_=\"Conv2DLSTM_AAE_PED1\", data_set=\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQZhjkL1NycS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '/content/drive/My Drive/Persistence1D/python')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from persistence1d import RunPersistence\n",
        "from reconstruct1d import RunReconstruction\n",
        "import math    \n",
        "\n",
        "from shapely.geometry import LineString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjBu8hyrfnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestVideoFile_ped1 = list((36,2))\n",
        "end = 200\n",
        "#TestVideoFile_ped1_ori = [[60,152], [50,175], [91,end], [31,168], [5,90], [1,100], [1,175], [1,94], [1,48], [1,140],   [70,165],   [130,end],   [1,156],   [1,end],   [138,end],   [123,end],   [1,47],   [54,120],    [64,138],    [45,175],    [31,end],    [16,107],    [8,165],    [50,171],    [40,135],    [77,144],    [10,122],    [105,end],    [1,15],    [175,end],    [1,180],    [1,52],  [5,165],    [1,121],    [86,end],   [15,108]]\n",
        "\n",
        "\n",
        "TestVideoFile_ped1 = [[60,152], [50,175], [91,end], [1,168], [5,139], [1,100], [1,175], [1,94], [1,48], [1,140],   [115,end],   [130,end],   [1,156],   [1,end],   [138,end],   [50, 60],   [1,47],   [54,120],    [64,138],    [45,175],    [31,end],    [16,107],    [8,end],    [1,171],    [40,135],    [77,end],    [10,122],    [105,end],    [1,15],    [175,end],    [1,180],    [1,52],  [1,175],    [1,121],    [86,end],   [15,108]]\n",
        "\n",
        "\n",
        "\n",
        "TestVideoFile_ped2 = list((12,2))\n",
        "TestVideoFile_ped2 = [[61,180],[95,180],[1,146],[31,180],[1,129],[1,159],[46,180],[1,180],[1,120],[1,150],[1,180],[88,180]]\n",
        "\n",
        "\n",
        "ped2_end_arr = [180, 180, 150, 180, 150, 180, 180, 180, 120, 150, 180, 180]\n",
        "def SortExtremaByPersistence(ExtremaAndPersistence):\n",
        "    #~ Sort the list of extrema by persistence.\n",
        "    #~ The original list from RunPersistence() is not guaranteed to be sorted,\n",
        "    #~ although it may appear sorted in many cases.\n",
        "    #~ This call to sorted() creates a new list. If you want to sort in-place, use ExtremaAndPersistence.sort()\n",
        "    SortedExtremaAndPersistence = sorted(ExtremaAndPersistence, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[0])\n",
        "    return SortedExtremaAndPersistence\n",
        "\n",
        "def GetMinima(ExtremaAndPersistence):\n",
        "    Minima = [t for t in ExtremaAndPersistence[::2]]\n",
        "    return Minima\n",
        "\n",
        "def GetMaxima(ExtremaAndPersistence):\n",
        "    Maxima = [t for t in ExtremaAndPersistence[1::2]]\n",
        "    return Maxima\n",
        "\n",
        "\n",
        "def GetIntersection(x, f, g):\n",
        "\n",
        "    plt.plot(x, f)\n",
        "    plt.plot(x, g)\n",
        "\n",
        "    first_line = LineString(np.column_stack((x, f)))\n",
        "    second_line = LineString(np.column_stack((x, g)))\n",
        "    intersection = first_line.intersection(second_line)\n",
        "    print(\"Intersection\")\n",
        "    #print(intersection)\n",
        "    x_list = []\n",
        "    if intersection.geom_type == 'MultiPoint':\n",
        "        plt.plot(*LineString(intersection).xy, 'o')\n",
        "        print(\"Multipoint\")\n",
        "        print(*LineString(intersection).xy[0])\n",
        "        for x_point in LineString(intersection).xy[0]:\n",
        "            x_list.append(x_point)\n",
        "    elif intersection.geom_type == 'Point':\n",
        "        plt.plot(*intersection.xy, 'o')\n",
        "        print(\"Point\")\n",
        "        print(*intersection.xy[0])\n",
        "        x_list.append(*intersection.xy[0])\n",
        "\n",
        "    x_arr = np.array(x_list)\n",
        "    #print(\"X_LIST\")\n",
        "    #print(x_arr)\n",
        "    return x_arr\n",
        "    #all(i >= 30 for i in g[])\n",
        "\n",
        "def find_overlap_rate(reference, detection):\n",
        "\n",
        "    overlap = 0\n",
        "    for x, y in detection:\n",
        "\n",
        "        print(\"Intersect x, y   =   \"+str(x)+\" , \"+str(y))\n",
        "        for p, q in reference:\n",
        "            print(\"Reference p, q   =   \"+str(p)+\" , \"+str(q))\n",
        "            if y > p and x < q:\n",
        "                overlap = overlap+ abs(min(y,q) - max(x,p))\n",
        "\n",
        "    print(\"Overlap: \"+str(overlap))\n",
        "\n",
        "    return overlap\n",
        "\n",
        "\n",
        "def persistence(InputData, dt, id, color):\n",
        "    if(dt == 1):\n",
        "        end = ped2_end_arr[id-1]\n",
        "    #~ Compute the extrema of the given data and their persistence.\n",
        "    ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "    #~ Keep only those extrema with a persistence larger than 0.5.\n",
        "    FilteredIndices = [t[0] for t in ExtremaAndPersistence if t[1] >= 0.1]\n",
        "\n",
        "    #~ This simple call is all you need to reconstruct a smooth function containing only the filtered extrema\n",
        "    SmoothData = RunReconstruction(InputData, FilteredIndices, 'biharmonic', 0.0000001)\n",
        "    #print(\"Smooth data \")\n",
        "    #print(SmoothData)\n",
        "\n",
        "    x = np.array([x for x in range(0, InputData.shape[0])])\n",
        "    \n",
        "    g = SmoothData\n",
        "    \n",
        "\n",
        "    ##add gt_tuples : TODO\n",
        "    \n",
        "    #print(intersections)\n",
        "    #~ Plot original and smoothed data\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(0, len(InputData)), InputData, label=\"Original Data\", color=\"blue\")\n",
        "    ax.plot(range(0, len(SmoothData)), SmoothData, label=\"Smooth Data\", color=\"magenta\")\n",
        "    ExtremaIndices = [t[0] for t in ExtremaAndPersistence]\n",
        "    \n",
        "    \"\"\"\n",
        "    sorted_extr = SortExtremaByPersistence(ExtremaAndPersistence)\n",
        "    minima = SortExtremaByPersistence(GetMinima(ExtremaAndPersistence))\n",
        "    maxima = SortExtremaByPersistence(GetMaxima(ExtremaAndPersistence))\n",
        "    print(\"###########Print All Extremas#################\")\n",
        "\n",
        "  \n",
        "    print([t for t in ExtremaAndPersistence])\n",
        "\n",
        "    print(\"###########Print Filtered Extremas#################\")\n",
        "\n",
        "    print([t for t in FilteredIndices])\n",
        "\n",
        "    print(\"###########Print Sorted Extremas#################\")\n",
        "\n",
        "    print([t for t in sorted_extr])\n",
        "\n",
        "    print(\"###########Print Minimas#################\")\n",
        "\n",
        "    print([t for t in minima])\n",
        "\n",
        "    print(\"###########Print Maximas#################\")\n",
        "\n",
        "    print([t for t in maxima])\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    gt_tuples = []\n",
        "    #ax.plot(ExtremaIndices, InputData[ExtremaIndices], marker='.', linestyle='')\n",
        "    ax.plot(FilteredIndices, InputData[FilteredIndices], marker='*', linestyle='')\n",
        "    ax.set(xlabel='data index', ylabel='data value')\n",
        "    #ax.set_aspect(1.0/ax.get_data_ratio()*0.2)\n",
        "    if dt == 1:\n",
        "        TestVideoFile_ped = TestVideoFile_ped1\n",
        "    elif dt == 2:\n",
        "        TestVideoFile_ped = TestVideoFile_ped2\n",
        "    plt.axvspan(TestVideoFile_ped[id-1][0]-1, TestVideoFile_ped[id-1][1]-1, alpha=0.5, color=color)\n",
        "    #plt.axhline(y=threshold_abs,linewidth=1, color='blue') \n",
        "\n",
        "\n",
        "    gt_tuples.append((TestVideoFile_ped[id-1][0]-1, TestVideoFile_ped[id-1][1]-1))\n",
        "    if dt == 1 and id == 5:\n",
        "        plt.axvspan(140-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((140-1, end-1))\n",
        "    elif dt == 1 and id == 6:\n",
        "        plt.axvspan(110-1, end-1, alpha=0.5, color=color) \n",
        "        gt_tuples.append((110-1, end-1))\n",
        "    if dt == 1 and id == 8:\n",
        "        plt.axvspan(150-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((150-1, end-1))\n",
        "    if dt == 1 and id == 16:\n",
        "        plt.axvspan(123-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((123-1, end-1))\n",
        "    if dt == 1 and id == 18:\n",
        "        plt.axvspan(160-1, 195-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((160-1, 195-1))\n",
        "    if dt == 1 and id == 22:\n",
        "        plt.axvspan(172-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((172-1, end-1))\n",
        "    elif dt == 1 and id == 29:\n",
        "        plt.axvspan(45-1, 113-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((45-1, 113-1))\n",
        "    elif dt == 1 and id == 32:\n",
        "        plt.axvspan(65-1, 115-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((65-1, 115-1))\n",
        "\n",
        "    gt_inverse_tuples = []\n",
        "\n",
        "    gti_start = 0\n",
        "    for p, q in gt_tuples:\n",
        "        if(p == gti_start):\n",
        "            gti_start = q\n",
        "            continue\n",
        "        gt_inverse_tuples.append((gti_start, p))\n",
        "        gti_start = q\n",
        "\n",
        "    if gti_start < end-1:\n",
        "        gt_inverse_tuples.append((gti_start, end-1))\n",
        "\n",
        "    print(\"GT Tuples: \"+str(gt_tuples))\n",
        "    print(\"GT Inverse Tuples: \"+str(gt_inverse_tuples))\n",
        "\n",
        "    TPR_ARR = []\n",
        "    FPR_ARR = []\n",
        "\n",
        "    for threshold_abs in np.arange(0.4, 1.05, 0.05):\n",
        "        f = np.ones(x.shape[0])*threshold_abs\n",
        "        intersections = [0]\n",
        "        intersections.extend(GetIntersection(x, f, g))\n",
        "        if(intersections[len(intersections)-1] < end -1):\n",
        "            intersections.extend([end-1])\n",
        "\n",
        "        intersections = np.array(intersections)\n",
        "        all_below_th = []\n",
        "        #all_above_th = []\n",
        "\n",
        "        for j, __ in enumerate(intersections):\n",
        "\n",
        "            if all(i <= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "                all_below_th.append((intersections[j], intersections[j+1]))\n",
        "                \n",
        "            if(end - 1 == math.floor(intersections[j+1])):\n",
        "                break\n",
        "            #if all(i >= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "            #    all_above_th.append((intersections[j], intersections[j+1]))\n",
        "\n",
        "        print(\"_________Find TPR FPR for Threshold : \"+str(threshold_abs)+\"__________\")\n",
        "        print(\"============Find TPR============\")\n",
        "        TPR = find_overlap_rate(gt_tuples, all_below_th)\n",
        "        print(\"============Find FPR============\")\n",
        "        FPR = find_overlap_rate(gt_inverse_tuples, all_below_th)\n",
        "\n",
        "        TPR_ARR.append(TPR)\n",
        "        FPR_ARR.append(FPR)\n",
        "    \n",
        "        print(\"TPR : \"+str(TPR))\n",
        "        print(\"FPR : \"+str(FPR))\n",
        "        print(\"\\n\\n\\n\")\n",
        "    \n",
        "    total_ref_gt = 0\n",
        "    for p, q in gt_tuples:\n",
        "        total_ref_gt = total_ref_gt+ abs(q-p)\n",
        "    print(\"Total GT: \"+str(total_ref_gt))\n",
        "    total_ref_gti = 0\n",
        "    for p, q in gt_inverse_tuples:\n",
        "        total_ref_gti = total_ref_gti+ abs(q-p)\n",
        "    print(\"Total GT Inv: \"+str(total_ref_gti))\n",
        "\n",
        "    ax.grid()\n",
        "    #ax.plot()\n",
        "\n",
        "    plt.legend()    \n",
        "    plt.show()\n",
        "\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "threshold = 0.9\n",
        "def fill_gt_ped1(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    print(sr.shape)\n",
        "    #zeros = np.zeros((conf.dim1,))\n",
        "    #print(zeros.shape)\n",
        "    #plt.plot(np.concatenate((zeros, sr)))\n",
        "    #persistence(sr, 1, id, color)\n",
        "    #yhat = savgol_filter(sr, 51, 3)\n",
        "    #TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(yhat, 1, id, color)\n",
        "    TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(sr, 1, id, color)\n",
        "    #plt.plot(sr)  \n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "\n",
        "\n",
        "def fill_gt_ped2(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    #plt.plot(np.concatenate((np.zeros((conf.dim1,)), sr)))\n",
        "    #persistence(sr, 2, id, color)\n",
        "    #yhat = savgol_filter(sr, 51, 3)\n",
        "    TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(sr, 2, id, color)\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXfjsTem0oM",
        "colab_type": "code",
        "outputId": "76c9f9c5-8175-4f54-b14b-295bb438c637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "def build_model_aae():\n",
        "  \n",
        "    model_enc = load_model(conf.cwdir+\"/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_dec = load_model(conf.cwdir+\"/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_disc = load_model(conf.cwdir+\"/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    #model_enc = load_model(\"/content/drive/My Drive/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_dec = load_model(\"/content/drive/My Drive/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_disc = load_model(\"/content/drive/My Drive/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    model_ae = Sequential()\n",
        "    model_ae.add(model_enc)\n",
        "    model_ae.add(model_dec)\n",
        "    \n",
        "    model_enc_disc = Sequential()\n",
        "    model_enc_disc.add(model_enc)\n",
        "    model_enc_disc.add(model_disc)\n",
        "    \n",
        "    return model_enc, model_dec, model_disc, model_ae, model_enc_disc\n",
        "\n",
        "model_enc, model_dec, model_disc, model_ae, model_enc_disc = build_model_aae()\n",
        "\n",
        "model_enc.summary()\n",
        "model_dec.summary()\n",
        "model_disc.summary()\n",
        "model_ae.summary()\n",
        "model_enc_disc.summary()\n",
        "\n",
        "model_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_enc_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_ae.compile(optimizer=Adam(lr=1e-4, decay=1e-5, epsilon=1e-6), loss=\"mse\")\n",
        "#\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/model.hdf5\"\n",
        "\n",
        "#model_ae = load_model(\"/content/drive/My Drive/VAE/model1.hdf5\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "def evaluate_dis(sequences, model, id, dt):\n",
        "    fooling_loss = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    sa = (fooling_loss - np.min(fooling_loss)) / (np.max(fooling_loss))\n",
        "    sr = 1.0 - sa\n",
        "\n",
        "    with open(join(\"/content/drive/My Drive/\", 'sr_score.csv'), mode='a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([float(x[0]) for x in sr])\n",
        "        f.close()\n",
        "\n",
        "    threshold_abs = threshold# np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        fill_gt_ped1(sr, id, 'red', threshold_abs)\n",
        "    elif dt == 2:\n",
        "        fill_gt_ped2(sr, id, 'red', threshold_abs)\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_dis(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "\n",
        "def get_clips(test):\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def mask_array(bg_list):\n",
        "\n",
        "    for p in range(0, bg_list.shape[0]):\n",
        "        for r in range(256):\n",
        "            for c in range(256):\n",
        "                if bg_list[p][r][c] > 0:\n",
        "                    bg_list[p][r][c] = 1\n",
        "                else:\n",
        "                    bg_list[p][r][c] = 0\n",
        "\n",
        "    bg_list[0, :, :] = 0\n",
        "    return bg_list\n",
        "\n",
        "def evaluate_ae(sequences, model, id, dt, bg_list=[], mask=1):\n",
        "  \n",
        "    sz = len(sequences)\n",
        "\n",
        "    reconstructed_sequences = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    #print(bg_list[10])\n",
        "\n",
        "    masked_bg = mask_array(bg_list)\n",
        "    bg_clips = get_clips(masked_bg)\n",
        "    bg_clips = prepend_10_clips(bg_clips)\n",
        "    sequences = np.reshape(sequences, (sequences.shape[0], sequences.shape[1], sequences.shape[2], sequences.shape[3]))\n",
        "    reconstructed_sequences = np.reshape(reconstructed_sequences, (reconstructed_sequences.shape[0], reconstructed_sequences.shape[1], reconstructed_sequences.shape[2], reconstructed_sequences.shape[3]))\n",
        "\n",
        "    if mask == 1:\n",
        "        print(\"==============With BG Masked==============\")\n",
        "        sq_masked = np.multiply(sequences, bg_clips)\n",
        "        rec_masked = np.multiply(reconstructed_sequences, bg_clips)\n",
        "    else:\n",
        "        print(\"==============Without BG Masked==============\")\n",
        "        sq_masked = sequences\n",
        "        rec_masked = reconstructed_sequences\n",
        "\n",
        "    #bg_weights = np.add(np.ones(bg_clips.shape), bg_clips)\n",
        "    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sq_masked[i], rec_masked[i])) for i in range(0,sz)])\n",
        "    #sequences_reconstruction_cost = np.array([np.linalg.norm(np.multiply(bg_weights[i],np.subtract(sequences[i], reconstructed_sequences[i]))) for i in range(0,sz)])\n",
        "\n",
        "\n",
        "    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / (np.max(sequences_reconstruction_cost))\n",
        "\n",
        "    \n",
        "    sr = 1 - sa\n",
        "    #threshold_abs = threshold#np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = fill_gt_ped1(sr, id, 'cyan')\n",
        "    elif dt == 2:\n",
        "        TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = fill_gt_ped2(sr, id, 'cyan')\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_ae(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "def load_input_data_list():\n",
        "    path = join(\"/content/drive/My Drive/\", 'sr_score.csv')\n",
        "    InputDataList = LoadData(path)\n",
        "    return InputDataList\n",
        "\n",
        "\n",
        "def get_persistance(InputData):\n",
        "  #~ This simple call is all you need to compute the extrema of the given data and their persistence.\n",
        "  ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "  #~ Keep only those extrema with a persistence larger than 10.\n",
        "  Filtered = [t for t in ExtremaAndPersistence if ExtremaAndPersistence[1] > 50]\n",
        "  print(ExtremaAndPersistence)\n",
        "  print(Filtered)\n",
        "  #~ Sort the list of extrema by persistence.\n",
        "  #Sorted = sorted(Filtered, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[1])\n",
        "\n",
        "  return Filtered\n",
        "\n",
        "ped1_or_2 = 2\n",
        "conf.reconfig(new_name=\"Conv2DLSTM_AAE_PED\"+str(ped1_or_2), batch_size=4, epochs=100, retrain=1)\n",
        "all_test_count = 0\n",
        "if ped1_or_2 == 1:\n",
        "    all_test_count = 36\n",
        "elif ped1_or_2 == 2:\n",
        "    all_test_count = 12\n",
        "\n",
        "def edge_detect(gray):\n",
        "    edges1 = np.uint8(gray)\n",
        "    # Using the Canny filter to get contours\n",
        "    #edges = cv2.Canny(edges1, 20, 30)\n",
        "    \n",
        "    # Using the Canny filter with different parameters\n",
        "    edges_high_thresh = cv2.Canny(edges1, 160, 200)\n",
        "    kernel = np.zeros((5,5),np.uint8)\n",
        "    kernel2 = np.zeros((2,2),np.uint8)\n",
        "\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(images, cv2.MORPH_CLOSE, kernel2)\n",
        "    # Stacking the images to print them together\n",
        "    # For comparison\n",
        "    #images = np.hstack((gray, edges, edges_high_thresh))\n",
        "\n",
        "    #kernel = np.ones((3,3), np.uint8) \n",
        "    #fg_mask = cv2.erode(edges_high_thresh, kernel, iterations=2)\n",
        "    # Display the resulting frame\n",
        "    #cv2_imshow(images)\n",
        "    return images\n",
        "\n",
        "    \n",
        "\n",
        "def connected_comp(img):\n",
        "\n",
        "    #find all your connected components (white blobs in your image)\n",
        "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
        "    #connectedComponentswithStats yields every seperated component with information on each of them, such as size\n",
        "    #the following part is just taking out the background which is also considered a component, but most of the time we don't want that.\n",
        "    sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
        "\n",
        "    # minimum size of particles we want to keep (number of pixels)\n",
        "    #here, it's a fixed value, but you can set it as you want, eg the mean of the sizes or whatever\n",
        "    min_size = 10  \n",
        "\n",
        "    #your answer image\n",
        "    img2 = np.zeros((output.shape))\n",
        "    #for every component in the image, you keep it only if it's above min_size\n",
        "    for i in range(0, nb_components):\n",
        "        if sizes[i] >= min_size:\n",
        "            img2[output == i + 1] = 255\n",
        "    return img2\n",
        "\n",
        "def get_background_subtr(test_case):\n",
        "    print(\"BGS show\" + str(np.array(test_case).shape))\n",
        "\n",
        "    subtractor = cv2.createBackgroundSubtractorMOG2(history=50, varThreshold=50, detectShadows=True)\n",
        "    bg_list = []\n",
        "    for frame in test_case:\n",
        "        frame = np.reshape(frame, (256, 256))*256\n",
        "        edge_mask = edge_detect(frame)\n",
        "        bg_mask = connected_comp(edge_mask)\n",
        "        bg_mask = subtractor.apply(bg_mask)\n",
        "        #edge_mask[0] = np.zeros(edge_mask[0].shape)\n",
        "        #kernel = np.ones((5,5), np.uint8) \n",
        "        #fg_mask = cv2.erode(mask, kernel, iterations=2)\n",
        "        #fg_mask = cv2.dilate(fg_mask, kernel, iterations=5)\n",
        "        fg_mask = bg_mask/255.0\n",
        "        #fg_mask = mask\n",
        "        #cv2_imshow(fg_mask)\n",
        "        #print(fg_mask)\n",
        "        bg_list.append(fg_mask)\n",
        "        #key = cv2.waitKey(30)\n",
        "        #if key == 27:\n",
        "        #    break\n",
        "    cv2_imshow(bg_list[100]*255.0)\n",
        "    #cv2.destroyAllWindows()\n",
        "    #print(bg_list)\n",
        "    return np.array(bg_list)\n",
        "\n",
        "\n",
        "def show_bgs(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    return get_background_subtr(test)\n",
        "\n",
        "\n",
        "TPR_FOR_CASES = []\n",
        "FPR_FOR_CASES = []\n",
        "total_ref_gt_arr = []\n",
        "total_ref_gti_arr = []\n",
        "#print(bg_list[10])\n",
        "for i in range(1,all_test_count):\n",
        "  if i < 10:\n",
        "    img_num = \"00\"+str(i)\n",
        "  elif i < 100:\n",
        "    img_num = \"0\"+str(i)\n",
        "  else:\n",
        "    img_num = str(i) \n",
        "\n",
        "  #if img_num == \"017\":\n",
        "  #  continue\n",
        "\n",
        "  bg_list = show_bgs(\"Test\"+img_num)\n",
        "  \n",
        "  \n",
        "  test_cases_dir = \"Test\"+img_num\n",
        "  test_cases = get_test_sequences(test_cases_dir)\n",
        "  test_cases = np.array(prepend_10_clips(test_cases))\n",
        "  print(\"Test cases shape: \"+str(np.array(test_cases).shape))\n",
        "  print(\"Test\"+img_num+\" data set loaded\")\n",
        "  #evaluate_dis(test_cases, model_enc_disc, i, 1)\n",
        "  #sorted_sr = get_persistance(np.array([x[0] for x in sr_dis], dtype=float))\n",
        "  #print(sorted_sr)\n",
        "  #break\n",
        "  TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = evaluate_ae(test_cases, model_ae, i, 2, bg_list, 1)\n",
        "  TPR_FOR_CASES.append(TPR_ARR)\n",
        "  FPR_FOR_CASES.append(FPR_ARR)\n",
        "  total_ref_gt_arr.append(total_ref_gt)\n",
        "  total_ref_gti_arr.append(total_ref_gti)\n",
        "  \n",
        "  #evaluate_ae(test_cases, model_ae, i, 1, bg_list, 0)\n",
        "\n",
        "  #sr_comb = (sr_dis + sr_ae)*0.5\n",
        "  #sr_comb = []\n",
        "\n",
        "  continue\n",
        "\n",
        "print(\"total_ref_gt_arr :\"+str(total_ref_gt_arr))\n",
        "print(\"total_ref_gti_arr :\"+str(total_ref_gti_arr))\n",
        "\n",
        "\n",
        "TPR_FOR_CASES = np.array(TPR_FOR_CASES)\n",
        "FPR_FOR_CASES = np.array(FPR_FOR_CASES)\n",
        "i=0\n",
        "j=0\n",
        "TPR_ADD = np.zeros((TPR_FOR_CASES[0].shape[0],))\n",
        "TP_REF_ADD = np.zeros(TPR_ADD.shape) \n",
        "for id, TPR_CASE in enumerate(TPR_FOR_CASES):\n",
        "\n",
        "    for i, __ in enumerate(TPR_CASE):\n",
        "        TPR_ADD[i] = TPR_ADD[i]+TPR_CASE[i]\n",
        "        print()\n",
        "        TP_REF_ADD[i] = TP_REF_ADD[i]+total_ref_gt_arr[id]\n",
        "\n",
        "\n",
        "id=0\n",
        "\n",
        "FPR_ADD = np.zeros((FPR_FOR_CASES[0].shape[0],))\n",
        "FP_REF_ADD = np.zeros(FPR_ADD.shape) \n",
        "for id, FPR_CASE in enumerate(FPR_FOR_CASES):\n",
        "\n",
        "    for j, __ in enumerate(FPR_CASE):\n",
        "        FPR_ADD[j] = FPR_ADD[j]+FPR_CASE[j]\n",
        "        FP_REF_ADD[j] = FP_REF_ADD[j]+total_ref_gti_arr[id]\n",
        "\n",
        "\n",
        "print(TP_REF_ADD)\n",
        "print(FP_REF_ADD)\n",
        "\n",
        "TPR_PLOT = np.divide(TPR_ADD, TP_REF_ADD)\n",
        "FPR_PLOT = np.divide(FPR_ADD, FP_REF_ADD)\n",
        "\n",
        "print(TPR_PLOT.shape)\n",
        "print(FPR_PLOT.shape)\n",
        "\n",
        "plt.plot(TPR_PLOT)\n",
        "plt.plot(FPR_PLOT)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6265ff3f6684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_enc_disc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_enc_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_aae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6265ff3f6684>\u001b[0m in \u001b[0;36mbuild_model_aae\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_gen_Conv2DLSTM_AAEep100\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LayerNormalization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_dec_Conv2DLSTM_AAEep100\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LayerNormalization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_dis_Conv2DLSTM_AAEep100\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LayerNormalization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/content/drive/My Drive/Conv2DLSTM_AAE_PED2/model_dec_Conv2DLSTM_AAEep100', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWetO3yHoYee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(FPR_PLOT, TPR_PLOT, 'b', label = \"FPR vs TPR\")\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "print(\"TPR\")\n",
        "print(TPR_PLOT)\n",
        "print(\"FPR\")\n",
        "print(FPR_PLOT)\n",
        "\n",
        "def integrate(x, y):\n",
        "    area = np.trapz(y=y, x=x, dx=0.05)\n",
        "    return area\n",
        "\n",
        "\n",
        "print(\"AUC = \"+str(integrate(FPR_PLOT, TPR_PLOT)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}