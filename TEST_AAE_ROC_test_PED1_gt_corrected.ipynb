{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST_AAE_ROC_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbk/Anomaly-Detection-System/blob/master/TEST_AAE_ROC_test_PED1_gt_corrected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVE526Rfl8ed",
        "colab_type": "code",
        "outputId": "e3d6a52b-7ffd-4e24-93a7-23fd17cdf42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "!pip install imageio\n",
        "!pip install qpsolvers\n",
        "!pip install shapely \n",
        "#!pip install tensorflow_datasets\n",
        "!pip install keras-layer-normalization\n",
        "from google.colab import drive\n",
        "#!pip install alive-progress\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.4)\n",
            "Collecting qpsolvers\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/10/b9a34d08edd4f492498dcb46c016cc02866a73132efb197c9d2865e21672/qpsolvers-1.1.tar.gz\n",
            "Collecting quadprog\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/f0/d4c8866f5d14dfa1a441439f5ce0d2680844651772129c431e78caadfe10/quadprog-0.1.7.tar.gz\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from quadprog->qpsolvers) (0.29.17)\n",
            "Building wheels for collected packages: qpsolvers, quadprog\n",
            "  Building wheel for qpsolvers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qpsolvers: filename=qpsolvers-1.1-cp36-none-any.whl size=16348 sha256=e00721d7fb8b48c6a54eb515bae88cdf9fcb5b35f43f4ec3d068cd69d51ea71b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/4b/c9/a05d1e02e4a031c89d46a3f5a68dadc6280999680ed01c6084\n",
            "  Building wheel for quadprog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quadprog: filename=quadprog-0.1.7-cp36-cp36m-linux_x86_64.whl size=307627 sha256=c462eef35ae450ed40981f1585c75ba824b66d04dc89bbb4205accdb7849d438\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/dd/b1/849989444c0a5930927b260663019b7da6cff864fc224c2747\n",
            "Successfully built qpsolvers quadprog\n",
            "Installing collected packages: quadprog, qpsolvers\n",
            "Successfully installed qpsolvers-1.1 quadprog-0.1.7\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Collecting keras-layer-normalization\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (1.18.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (2.10.0)\n",
            "Building wheels for collected packages: keras-layer-normalization\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=3fb3eecf671afcbd0bf8bff29b6e573c2aa6662f2e06db2e5aacc89f128c891b\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "Successfully built keras-layer-normalization\n",
            "Installing collected packages: keras-layer-normalization\n",
            "Successfully installed keras-layer-normalization-0.14.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drSah6JgmAU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import skimage\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "#import keras\n",
        "import argparse\n",
        "from os.path import dirname\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\t\n",
        "import statistics\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "#from progress.bar import IncrementalBar\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D, Dropout, Activation, InputLayer\n",
        "from keras.optimizers import Adam\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJ798qPmI9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepend_10_clips(test_case):\n",
        "    \n",
        "    test_aug = np.zeros((test_case.shape[0]+10, test_case.shape[1:]))\n",
        "    for i in range(0,10):\n",
        "        test_aug[2*i] = test_case[i]\n",
        "        test_aug[2*i+1] = test_case[i]\n",
        "\n",
        "\n",
        "    test_aug.extend(test_cases[10:])\n",
        "    return test_aug\n",
        "\n",
        "\n",
        "def get_single_test(single_test_path):\n",
        "    \n",
        "    sz = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "          sz = sz +1\n",
        "    test = np.zeros(shape=(sz, conf.dim2, conf.dim3, conf.dim4))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "            #print(\"img path: \"+join(single_test_path, f))\n",
        "            img = Image.open(join(single_test_path, f)).resize((conf.dim2, conf.dim3))\n",
        "            #cv2_imshow(np.array(img,dtype=np.float32))\n",
        "            #cv2.waitKey(0)\n",
        "            img = np.array(img, dtype=np.float32) / 256\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    \n",
        "    prepend_10_clips(np.array(test))\n",
        "    return test\n",
        "\n",
        "def get_test_sequences(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    print(\"Test case loaded\")\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgtzofEvwlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    def __init__(self, data_dir_, cwdir_name_, data_set):\n",
        "        self.data_set_name = data_set\n",
        "        self.data_dir = data_dir_\n",
        "        self.data_set_dir = join(self.data_dir, data_set)\n",
        "        self.cwdir_name = cwdir_name_\n",
        "        self.cwdir = join(self.data_dir,self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "        if not os.path.exists(self.cwdir):\n",
        "            os.mkdir(self.cwdir)\n",
        "            os.mkdir(self.run_data)\n",
        "    \n",
        "        if not os.path.exists(self.run_data):\n",
        "            #shutil.rmtree(self.run_data)\n",
        "            os.mkdir(self.run_data)\n",
        "            os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "        self.DATASET_PATH = join(self.data_set_dir,\"Train/\")\n",
        "        self.TEST_DIR = join(self.data_set_dir,\"Test/\")\n",
        "        self.BATCH_SIZE = 2\n",
        "        self.EPOCHS = 50\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = 0\n",
        "        self.dim1 = 10\n",
        "        self.dim2 = 256\n",
        "        self.dim3 = 256\n",
        "        self.dim4 = 1\n",
        "        self.latent_dim = 327680\n",
        "\n",
        "\n",
        "    def reconfig(self, new_name, batch_size = 4, epochs = 5, retrain = 0):\n",
        "        self.cwdir_name = new_name\n",
        "        self.cwdir = join(self.data_dir, self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.EPOCHS = epochs\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = retrain\n",
        "        if retrain == 0:\n",
        "            print(\"Configuring train from scratch\")\n",
        "            if not os.path.exists(self.cwdir):\n",
        "                os.mkdir(self.cwdir)\n",
        "                os.mkdir(self.run_data)\n",
        "    \n",
        "            if os.path.exists(self.run_data):\n",
        "                shutil.rmtree(self.run_data)\n",
        "                os.mkdir(self.run_data)\n",
        "                os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "conf = Config(data_dir_=\"/content/drive/My Drive/\", cwdir_name_=\"Conv2DLSTM_AAE_PED1\", data_set=\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQZhjkL1NycS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '/content/drive/My Drive/Persistence1D/python')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from persistence1d import RunPersistence\n",
        "from reconstruct1d import RunReconstruction\n",
        "import math    \n",
        "\n",
        "from shapely.geometry import LineString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjBu8hyrfnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestVideoFile_ped1 = list((36,2))\n",
        "end = 200\n",
        "#TestVideoFile_ped1_ori = [[60,152], [50,175], [91,end], [31,168], [5,90], [1,100], [1,175], [1,94], [1,48], [1,140],   [70,165],   [130,end],   [1,156],   [1,end],   [138,end],   [123,end],   [1,47],   [54,120],    [64,138],    [45,175],    [31,end],    [16,107],    [8,165],    [50,171],    [40,135],    [77,144],    [10,122],    [105,end],    [1,15],    [175,end],    [1,180],    [1,52],  [5,165],    [1,121],    [86,end],   [15,108]]\n",
        "\n",
        "\n",
        "TestVideoFile_ped1 = [[60,152], [50,175], [91,end], [1,168], [5,139], [1,100], [1,175], [1,94], [1,48], [1,140],   [115,end],   [130,end],   [1,156],   [1,end],   [138,end],   [50, 60],   [1,47],   [54,120],    [64,138],    [45,175],    [31,end],    [16,107],    [8,end],    [1,171],    [40,135],    [77,end],    [10,122],    [105,end],    [1,15],    [175,end],    [1,180],    [1,52],  [1,175],    [1,121],    [86,end],   [15,108]]\n",
        "\n",
        "\n",
        "\n",
        "def SortExtremaByPersistence(ExtremaAndPersistence):\n",
        "    #~ Sort the list of extrema by persistence.\n",
        "    #~ The original list from RunPersistence() is not guaranteed to be sorted,\n",
        "    #~ although it may appear sorted in many cases.\n",
        "    #~ This call to sorted() creates a new list. If you want to sort in-place, use ExtremaAndPersistence.sort()\n",
        "    SortedExtremaAndPersistence = sorted(ExtremaAndPersistence, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[0])\n",
        "    return SortedExtremaAndPersistence\n",
        "\n",
        "def GetMinima(ExtremaAndPersistence):\n",
        "    Minima = [t for t in ExtremaAndPersistence[::2]]\n",
        "    return Minima\n",
        "\n",
        "def GetMaxima(ExtremaAndPersistence):\n",
        "    Maxima = [t for t in ExtremaAndPersistence[1::2]]\n",
        "    return Maxima\n",
        "\n",
        "\n",
        "def GetIntersection(x, f, g):\n",
        "\n",
        "    plt.plot(x, f)\n",
        "    plt.plot(x, g)\n",
        "\n",
        "    first_line = LineString(np.column_stack((x, f)))\n",
        "    second_line = LineString(np.column_stack((x, g)))\n",
        "    intersection = first_line.intersection(second_line)\n",
        "    print(\"Intersection\")\n",
        "    #print(intersection)\n",
        "    x_list = []\n",
        "    if intersection.geom_type == 'MultiPoint':\n",
        "        plt.plot(*LineString(intersection).xy, 'o')\n",
        "        print(\"Multipoint\")\n",
        "        print(*LineString(intersection).xy[0])\n",
        "        for x_point in LineString(intersection).xy[0]:\n",
        "            x_list.append(x_point)\n",
        "    elif intersection.geom_type == 'Point':\n",
        "        plt.plot(*intersection.xy, 'o')\n",
        "        print(\"Point\")\n",
        "        print(*intersection.xy[0])\n",
        "        x_list.append(*intersection.xy[0])\n",
        "\n",
        "    x_arr = np.array(x_list)\n",
        "    #print(\"X_LIST\")\n",
        "    #print(x_arr)\n",
        "    return x_arr\n",
        "    #all(i >= 30 for i in g[])\n",
        "\n",
        "def find_overlap_rate(reference, detection):\n",
        "\n",
        "    overlap = 0\n",
        "    for x, y in detection:\n",
        "\n",
        "        print(\"Intersect x, y   =   \"+str(x)+\" , \"+str(y))\n",
        "        for p, q in reference:\n",
        "            print(\"Reference p, q   =   \"+str(p)+\" , \"+str(q))\n",
        "            if y > p and x < q:\n",
        "                overlap = overlap+ abs(min(y,q) - max(x,p))\n",
        "\n",
        "    print(\"Overlap: \"+str(overlap))\n",
        "\n",
        "    return overlap\n",
        "\n",
        "\n",
        "def persistence(InputData, dt, id, color):\n",
        "    #~ Compute the extrema of the given data and their persistence.\n",
        "    ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "    #~ Keep only those extrema with a persistence larger than 0.5.\n",
        "    FilteredIndices = [t[0] for t in ExtremaAndPersistence if t[1] >= 0.1]\n",
        "\n",
        "    #~ This simple call is all you need to reconstruct a smooth function containing only the filtered extrema\n",
        "    SmoothData = RunReconstruction(InputData, FilteredIndices, 'biharmonic', 0.0000001)\n",
        "    #print(\"Smooth data \")\n",
        "    #print(SmoothData)\n",
        "\n",
        "    x = np.array([x for x in range(0, InputData.shape[0])])\n",
        "    \n",
        "    g = SmoothData\n",
        "    \n",
        "\n",
        "    ##add gt_tuples : TODO\n",
        "    \n",
        "    #print(intersections)\n",
        "    #~ Plot original and smoothed data\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(0, len(InputData)), InputData, label=\"Original Data\", color=\"blue\")\n",
        "    ax.plot(range(0, len(SmoothData)), SmoothData, label=\"Smooth Data\", color=\"orange\")\n",
        "    ExtremaIndices = [t[0] for t in ExtremaAndPersistence]\n",
        "    \n",
        "    \"\"\"\n",
        "    sorted_extr = SortExtremaByPersistence(ExtremaAndPersistence)\n",
        "    minima = SortExtremaByPersistence(GetMinima(ExtremaAndPersistence))\n",
        "    maxima = SortExtremaByPersistence(GetMaxima(ExtremaAndPersistence))\n",
        "    print(\"###########Print All Extremas#################\")\n",
        "\n",
        "  \n",
        "    print([t for t in ExtremaAndPersistence])\n",
        "\n",
        "    print(\"###########Print Filtered Extremas#################\")\n",
        "\n",
        "    print([t for t in FilteredIndices])\n",
        "\n",
        "    print(\"###########Print Sorted Extremas#################\")\n",
        "\n",
        "    print([t for t in sorted_extr])\n",
        "\n",
        "    print(\"###########Print Minimas#################\")\n",
        "\n",
        "    print([t for t in minima])\n",
        "\n",
        "    print(\"###########Print Maximas#################\")\n",
        "\n",
        "    print([t for t in maxima])\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    gt_tuples = []\n",
        "    ax.plot(ExtremaIndices, InputData[ExtremaIndices], marker='.', linestyle='')\n",
        "    ax.plot(FilteredIndices, InputData[FilteredIndices], marker='*', linestyle='')\n",
        "    ax.set(xlabel='data index', ylabel='data value')\n",
        "    #ax.set_aspect(1.0/ax.get_data_ratio()*0.2)\n",
        "    plt.axvspan(TestVideoFile_ped1[id-1][0]-1, TestVideoFile_ped1[id-1][1]-1, alpha=0.5, color=color)\n",
        "    #plt.axhline(y=threshold_abs,linewidth=1, color='blue') \n",
        "\n",
        "\n",
        "    gt_tuples.append((TestVideoFile_ped1[id-1][0]-1, TestVideoFile_ped1[id-1][1]-1))\n",
        "    if dt == 1 and id == 5:\n",
        "        plt.axvspan(140-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((140-1, end-1))\n",
        "    elif dt == 1 and id == 6:\n",
        "        plt.axvspan(110-1, end-1, alpha=0.5, color=color) \n",
        "        gt_tuples.append((110-1, end-1))\n",
        "    if dt == 1 and id == 8:\n",
        "        plt.axvspan(150-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((150-1, end-1))\n",
        "    if dt == 1 and id == 16:\n",
        "        plt.axvspan(123-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((123-1, end-1))\n",
        "    if dt == 1 and id == 18:\n",
        "        plt.axvspan(160-1, 195-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((160-1, 195-1))\n",
        "    if dt == 1 and id == 22:\n",
        "        plt.axvspan(172-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((172-1, end-1))\n",
        "    elif dt == 1 and id == 29:\n",
        "        plt.axvspan(45-1, 113-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((45-1, 113-1))\n",
        "    elif dt == 1 and id == 32:\n",
        "        plt.axvspan(65-1, 115-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((65-1, 115-1))\n",
        "\n",
        "    gt_inverse_tuples = []\n",
        "\n",
        "    gti_start = 0\n",
        "    for p, q in gt_tuples:\n",
        "        if(p == gti_start):\n",
        "            gti_start = q\n",
        "            continue\n",
        "        gt_inverse_tuples.append((gti_start, p))\n",
        "        gti_start = q\n",
        "\n",
        "    if gti_start < end-1:\n",
        "        gt_inverse_tuples.append((gti_start, end-1))\n",
        "\n",
        "    print(\"GT Tuples: \"+str(gt_tuples))\n",
        "    print(\"GT Inverse Tuples: \"+str(gt_inverse_tuples))\n",
        "\n",
        "    TPR_ARR = []\n",
        "    FPR_ARR = []\n",
        "\n",
        "    for threshold_abs in np.arange(0.4, 1.05, 0.05):\n",
        "        f = np.ones(x.shape[0])*threshold_abs\n",
        "        intersections = [0]\n",
        "        intersections.extend(GetIntersection(x, f, g))\n",
        "        if(intersections[len(intersections)-1] < end -1):\n",
        "            intersections.extend([end-1])\n",
        "\n",
        "        intersections = np.array(intersections)\n",
        "        all_below_th = []\n",
        "        #all_above_th = []\n",
        "\n",
        "        for j, __ in enumerate(intersections):\n",
        "\n",
        "            if all(i <= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "                all_below_th.append((intersections[j], intersections[j+1]))\n",
        "                \n",
        "            if(end - 1 == math.floor(intersections[j+1])):\n",
        "                break\n",
        "            #if all(i >= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "            #    all_above_th.append((intersections[j], intersections[j+1]))\n",
        "\n",
        "        print(\"_________Find TPR FPR for Threshold : \"+str(threshold_abs)+\"__________\")\n",
        "        print(\"============Find TPR============\")\n",
        "        TPR = find_overlap_rate(gt_tuples, all_below_th)\n",
        "        print(\"============Find FPR============\")\n",
        "        FPR = find_overlap_rate(gt_inverse_tuples, all_below_th)\n",
        "\n",
        "        TPR_ARR.append(TPR)\n",
        "        FPR_ARR.append(FPR)\n",
        "    \n",
        "        print(\"TPR : \"+str(TPR))\n",
        "        print(\"FPR : \"+str(FPR))\n",
        "        print(\"\\n\\n\\n\")\n",
        "    \n",
        "    total_ref_gt = 0\n",
        "    for p, q in gt_tuples:\n",
        "        total_ref_gt = total_ref_gt+ abs(q-p)\n",
        "    print(\"Total GT: \"+str(total_ref_gt))\n",
        "    total_ref_gti = 0\n",
        "    for p, q in gt_inverse_tuples:\n",
        "        total_ref_gti = total_ref_gti+ abs(q-p)\n",
        "    print(\"Total GT Inv: \"+str(total_ref_gti))\n",
        "\n",
        "    ax.grid()\n",
        "    #ax.plot()\n",
        "\n",
        "    plt.legend()    \n",
        "    plt.show()\n",
        "\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "threshold = 0.9\n",
        "def fill_gt_ped1(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    print(sr.shape)\n",
        "    #zeros = np.zeros((conf.dim1,))\n",
        "    #print(zeros.shape)\n",
        "    #plt.plot(np.concatenate((zeros, sr)))\n",
        "    #persistence(sr, 1, id, color)\n",
        "    #yhat = savgol_filter(sr, 51, 3)\n",
        "    #TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(yhat, 1, id, color)\n",
        "    TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(sr, 1, id, color)\n",
        "    #plt.plot(sr)  \n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "\n",
        "\n",
        "TestVideoFile_ped2 = list((12,2))\n",
        "TestVideoFile_ped2 = [[61,180],[95,180],[1,146],[31,180],[1,129],[1,159],[46,180],[1,180],[1,120],[1,150],[1,180],[88,180]]\n",
        "\n",
        "def fill_gt_ped2(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    #plt.plot(np.concatenate((np.zeros((conf.dim1,)), sr)))\n",
        "    persistence(sr, 2, id, color)\n",
        "    yhat = savgol_filter(sr, 51, 3)\n",
        "    TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = persistence(yhat, 2, id, color)\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXfjsTem0oM",
        "colab_type": "code",
        "outputId": "4b922954-7a6b-4f63-eb8f-591df99b1f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def build_model_aae():\n",
        "  \n",
        "    model_enc = load_model(conf.cwdir+\"/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_dec = load_model(conf.cwdir+\"/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_disc = load_model(conf.cwdir+\"/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    #model_enc = load_model(\"/content/drive/My Drive/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_dec = load_model(\"/content/drive/My Drive/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_disc = load_model(\"/content/drive/My Drive/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    model_ae = Sequential()\n",
        "    model_ae.add(model_enc)\n",
        "    model_ae.add(model_dec)\n",
        "    \n",
        "    model_enc_disc = Sequential()\n",
        "    model_enc_disc.add(model_enc)\n",
        "    model_enc_disc.add(model_disc)\n",
        "    \n",
        "    return model_enc, model_dec, model_disc, model_ae, model_enc_disc\n",
        "\n",
        "model_enc, model_dec, model_disc, model_ae, model_enc_disc = build_model_aae()\n",
        "\n",
        "model_enc.summary()\n",
        "model_dec.summary()\n",
        "model_disc.summary()\n",
        "model_ae.summary()\n",
        "model_enc_disc.summary()\n",
        "\n",
        "model_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_enc_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_ae.compile(optimizer=Adam(lr=1e-4, decay=1e-5, epsilon=1e-6), loss=\"mse\")\n",
        "#\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/model.hdf5\"\n",
        "\n",
        "#model_ae = load_model(\"/content/drive/My Drive/VAE/model1.hdf5\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "def evaluate_dis(sequences, model, id, dt):\n",
        "    fooling_loss = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    sa = (fooling_loss - np.min(fooling_loss)) / (np.max(fooling_loss))\n",
        "    sr = 1.0 - sa\n",
        "\n",
        "    with open(join(\"/content/drive/My Drive/\", 'sr_score.csv'), mode='a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([float(x[0]) for x in sr])\n",
        "        f.close()\n",
        "\n",
        "    threshold_abs = threshold# np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        fill_gt_ped1(sr, id, 'red', threshold_abs)\n",
        "    elif dt == 2:\n",
        "        fill_gt_ped2(sr, id, 'red', threshold_abs)\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_dis(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "\n",
        "def get_clips(test):\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def mask_array(bg_list):\n",
        "\n",
        "    for p in range(0, bg_list.shape[0]):\n",
        "        for r in range(256):\n",
        "            for c in range(256):\n",
        "                if bg_list[p][r][c] > 0:\n",
        "                    bg_list[p][r][c] = 1\n",
        "                else:\n",
        "                    bg_list[p][r][c] = 0\n",
        "\n",
        "    bg_list[0, :, :] = 0\n",
        "    return bg_list\n",
        "\n",
        "def evaluate_ae(sequences, model, id, dt, bg_list=[], mask=1):\n",
        "  \n",
        "    sz = len(sequences)\n",
        "\n",
        "    reconstructed_sequences = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    #print(bg_list[10])\n",
        "\n",
        "    masked_bg = mask_array(bg_list)\n",
        "    bg_clips = get_clips(masked_bg)\n",
        "    \n",
        "    sequences = np.reshape(sequences, (sequences.shape[0], sequences.shape[1], sequences.shape[2], sequences.shape[3]))\n",
        "    reconstructed_sequences = np.reshape(reconstructed_sequences, (reconstructed_sequences.shape[0], reconstructed_sequences.shape[1], reconstructed_sequences.shape[2], reconstructed_sequences.shape[3]))\n",
        "\n",
        "    if mask == 1:\n",
        "        print(\"==============With BG Masked==============\")\n",
        "        sq_masked = np.multiply(sequences, bg_clips)\n",
        "        rec_masked = np.multiply(reconstructed_sequences, bg_clips)\n",
        "    else:\n",
        "        print(\"==============Without BG Masked==============\")\n",
        "        sq_masked = sequences\n",
        "        rec_masked = reconstructed_sequences\n",
        "\n",
        "    #bg_weights = np.add(np.ones(bg_clips.shape), bg_clips)\n",
        "    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sq_masked[i], rec_masked[i])) for i in range(0,sz)])\n",
        "    #sequences_reconstruction_cost = np.array([np.linalg.norm(np.multiply(bg_weights[i],np.subtract(sequences[i], reconstructed_sequences[i]))) for i in range(0,sz)])\n",
        "\n",
        "\n",
        "    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / (np.max(sequences_reconstruction_cost))\n",
        "\n",
        "    \n",
        "    sr = 1 - sa\n",
        "    #threshold_abs = threshold#np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = fill_gt_ped1(sr, id, 'orange')\n",
        "    elif dt == 2:\n",
        "        TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = fill_gt_ped2(sr, id, 'orange')\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_ae(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "    return TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti\n",
        "\n",
        "def load_input_data_list():\n",
        "    path = join(\"/content/drive/My Drive/\", 'sr_score.csv')\n",
        "    InputDataList = LoadData(path)\n",
        "    return InputDataList\n",
        "\n",
        "\n",
        "def get_persistance(InputData):\n",
        "  #~ This simple call is all you need to compute the extrema of the given data and their persistence.\n",
        "  ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "  #~ Keep only those extrema with a persistence larger than 10.\n",
        "  Filtered = [t for t in ExtremaAndPersistence if ExtremaAndPersistence[1] > 50]\n",
        "  print(ExtremaAndPersistence)\n",
        "  print(Filtered)\n",
        "  #~ Sort the list of extrema by persistence.\n",
        "  #Sorted = sorted(Filtered, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[1])\n",
        "\n",
        "  return Filtered\n",
        "\n",
        "\n",
        "conf.reconfig(new_name=\"Conv2DLSTM_AAE_PED1\", batch_size=4, epochs=100, retrain=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def edge_detect(gray):\n",
        "    edges1 = np.uint8(gray)\n",
        "    # Using the Canny filter to get contours\n",
        "    #edges = cv2.Canny(edges1, 20, 30)\n",
        "    \n",
        "    # Using the Canny filter with different parameters\n",
        "    edges_high_thresh = cv2.Canny(edges1, 160, 200)\n",
        "    kernel = np.zeros((5,5),np.uint8)\n",
        "    kernel2 = np.zeros((2,2),np.uint8)\n",
        "\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(images, cv2.MORPH_CLOSE, kernel2)\n",
        "    # Stacking the images to print them together\n",
        "    # For comparison\n",
        "    #images = np.hstack((gray, edges, edges_high_thresh))\n",
        "\n",
        "    #kernel = np.ones((3,3), np.uint8) \n",
        "    #fg_mask = cv2.erode(edges_high_thresh, kernel, iterations=2)\n",
        "    # Display the resulting frame\n",
        "    #cv2_imshow(images)\n",
        "    return images\n",
        "\n",
        "    \n",
        "\n",
        "def connected_comp(img):\n",
        "\n",
        "    #find all your connected components (white blobs in your image)\n",
        "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
        "    #connectedComponentswithStats yields every seperated component with information on each of them, such as size\n",
        "    #the following part is just taking out the background which is also considered a component, but most of the time we don't want that.\n",
        "    sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
        "\n",
        "    # minimum size of particles we want to keep (number of pixels)\n",
        "    #here, it's a fixed value, but you can set it as you want, eg the mean of the sizes or whatever\n",
        "    min_size = 10  \n",
        "\n",
        "    #your answer image\n",
        "    img2 = np.zeros((output.shape))\n",
        "    #for every component in the image, you keep it only if it's above min_size\n",
        "    for i in range(0, nb_components):\n",
        "        if sizes[i] >= min_size:\n",
        "            img2[output == i + 1] = 255\n",
        "    return img2\n",
        "\n",
        "def get_background_subtr(test_case):\n",
        "    print(\"BGS show\" + str(np.array(test_case).shape))\n",
        "\n",
        "    subtractor = cv2.createBackgroundSubtractorMOG2(history=50, varThreshold=50, detectShadows=True)\n",
        "    bg_list = []\n",
        "    for frame in test_case:\n",
        "        frame = np.reshape(frame, (256, 256))*256\n",
        "        #edge_mask = edge_detect(frame)\n",
        "        #bg_mask = connected_comp(frame)\n",
        "        bg_mask = subtractor.apply(frame)\n",
        "        bg_mask = connected_comp(bg_mask)\n",
        "        #edge_mask[0] = np.zeros(edge_mask[0].shape)\n",
        "        #kernel = np.ones((5,5), np.uint8) \n",
        "        #fg_mask = cv2.erode(mask, kernel, iterations=2)\n",
        "        #fg_mask = cv2.dilate(fg_mask, kernel, iterations=5)\n",
        "        fg_mask = bg_mask/255.0\n",
        "        #fg_mask = mask\n",
        "        #cv2_imshow(fg_mask)\n",
        "        #print(fg_mask)\n",
        "        bg_list.append(fg_mask)\n",
        "        #key = cv2.waitKey(30)\n",
        "        #if key == 27:\n",
        "        #    break\n",
        "    cv2_imshow(bg_list[100]*255.0)\n",
        "    #cv2.destroyAllWindows()\n",
        "    #print(bg_list)\n",
        "    return np.array(bg_list)\n",
        "\n",
        "\n",
        "def show_bgs(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    return get_background_subtr(test)\n",
        "\n",
        "\n",
        "TPR_FOR_CASES = []\n",
        "FPR_FOR_CASES = []\n",
        "total_ref_gt_arr = []\n",
        "total_ref_gti_arr = []\n",
        "#print(bg_list[10])\n",
        "for i in range(1,37):\n",
        "  if i < 10:\n",
        "    img_num = \"00\"+str(i)\n",
        "  elif i < 100:\n",
        "    img_num = \"0\"+str(i)\n",
        "  else:\n",
        "    img_num = str(i) \n",
        "\n",
        "  #if img_num == \"017\":\n",
        "  #  continue\n",
        "\n",
        "  bg_list = show_bgs(\"Test\"+img_num)\n",
        "  \n",
        "  \n",
        "  test_cases_dir = \"Test\"+img_num\n",
        "  test_cases = get_test_sequences(test_cases_dir)\n",
        "  test_cases = np.array(prepend_10_clips(test_cases))\n",
        "  print(\"Test cases shape: \"+str(np.array(test_cases).shape))\n",
        "  print(\"Test\"+img_num+\" data set loaded\")\n",
        "  #evaluate_dis(test_cases, model_enc_disc, i, 1)\n",
        "  #sorted_sr = get_persistance(np.array([x[0] for x in sr_dis], dtype=float))\n",
        "  #print(sorted_sr)\n",
        "  #break\n",
        "  TPR_ARR, FPR_ARR, total_ref_gt, total_ref_gti = evaluate_ae(test_cases, model_ae, i, 1, bg_list, 1)\n",
        "  TPR_FOR_CASES.append(TPR_ARR)\n",
        "  FPR_FOR_CASES.append(FPR_ARR)\n",
        "  total_ref_gt_arr.append(total_ref_gt)\n",
        "  total_ref_gti_arr.append(total_ref_gti)\n",
        "  \n",
        "  #evaluate_ae(test_cases, model_ae, i, 1, bg_list, 0)\n",
        "\n",
        "  #sr_comb = (sr_dis + sr_ae)*0.5\n",
        "  #sr_comb = []\n",
        "\n",
        "  continue\n",
        "\n",
        "print(\"total_ref_gt_arr :\"+str(total_ref_gt_arr))\n",
        "print(\"total_ref_gti_arr :\"+str(total_ref_gti_arr))\n",
        "\n",
        "\n",
        "TPR_FOR_CASES = np.array(TPR_FOR_CASES)\n",
        "FPR_FOR_CASES = np.array(FPR_FOR_CASES)\n",
        "i=0\n",
        "j=0\n",
        "TPR_ADD = np.zeros((TPR_FOR_CASES[0].shape[0],))\n",
        "TP_REF_ADD = np.zeros(TPR_ADD.shape) \n",
        "for id, TPR_CASE in enumerate(TPR_FOR_CASES):\n",
        "\n",
        "    for i, __ in enumerate(TPR_CASE):\n",
        "        TPR_ADD[i] = TPR_ADD[i]+TPR_CASE[i]\n",
        "        print()\n",
        "        TP_REF_ADD[i] = TP_REF_ADD[i]+total_ref_gt_arr[id]\n",
        "\n",
        "\n",
        "id=0\n",
        "\n",
        "FPR_ADD = np.zeros((FPR_FOR_CASES[0].shape[0],))\n",
        "FP_REF_ADD = np.zeros(FPR_ADD.shape) \n",
        "for id, FPR_CASE in enumerate(FPR_FOR_CASES):\n",
        "\n",
        "    for j, __ in enumerate(FPR_CASE):\n",
        "        FPR_ADD[j] = FPR_ADD[j]+FPR_CASE[j]\n",
        "        FP_REF_ADD[j] = FP_REF_ADD[j]+total_ref_gti_arr[id]\n",
        "\n",
        "\n",
        "print(TP_REF_ADD)\n",
        "print(FP_REF_ADD)\n",
        "\n",
        "TPR_PLOT = np.divide(TPR_ADD, TP_REF_ADD)\n",
        "FPR_PLOT = np.divide(FPR_ADD, FP_REF_ADD)\n",
        "\n",
        "print(TPR_PLOT.shape)\n",
        "print(FPR_PLOT.shape)\n",
        "\n",
        "plt.plot(TPR_PLOT)\n",
        "plt.plot(FPR_PLOT)\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_7 (TimeDist (None, 10, 64, 64, 128)   15616     \n",
            "_________________________________________________________________\n",
            "layer_normalization_9 (Layer (None, 10, 64, 64, 128)   256       \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 10, 32, 32, 64)    204864    \n",
            "_________________________________________________________________\n",
            "layer_normalization_10 (Laye (None, 10, 32, 32, 64)    128       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_5 (ConvLSTM2D)  (None, 10, 32, 32, 64)    295168    \n",
            "_________________________________________________________________\n",
            "layer_normalization_11 (Laye (None, 10, 32, 32, 64)    128       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_6 (ConvLSTM2D)  (None, 10, 32, 32, 32)    110720    \n",
            "_________________________________________________________________\n",
            "layer_normalization_12 (Laye (None, 10, 32, 32, 32)    64        \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 327680)            0         \n",
            "=================================================================\n",
            "Total params: 626,944\n",
            "Trainable params: 626,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_3 (Reshape)          (None, 10, 32, 32, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_7 (ConvLSTM2D)  (None, 10, 32, 32, 32)    73856     \n",
            "_________________________________________________________________\n",
            "layer_normalization_13 (Laye (None, 10, 32, 32, 32)    64        \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_8 (ConvLSTM2D)  (None, 10, 32, 32, 64)    221440    \n",
            "_________________________________________________________________\n",
            "layer_normalization_14 (Laye (None, 10, 32, 32, 64)    128       \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 10, 64, 64, 64)    102464    \n",
            "_________________________________________________________________\n",
            "layer_normalization_15 (Laye (None, 10, 64, 64, 64)    128       \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 10, 256, 256, 128) 991360    \n",
            "_________________________________________________________________\n",
            "layer_normalization_16 (Laye (None, 10, 256, 256, 128) 256       \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 10, 256, 256, 1)   15489     \n",
            "=================================================================\n",
            "Total params: 1,405,185\n",
            "Trainable params: 1,405,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_4 (Reshape)          (None, 10, 32, 32, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, 10, 16, 16, 16)    4624      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 5, 8, 8, 16)       0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 5121      \n",
            "=================================================================\n",
            "Total params: 9,745\n",
            "Trainable params: 0\n",
            "Non-trainable params: 9,745\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_5 (Sequential)    (None, 327680)            626944    \n",
            "_________________________________________________________________\n",
            "sequential_6 (Sequential)    (None, 10, 256, 256, 1)   1405185   \n",
            "=================================================================\n",
            "Total params: 2,032,129\n",
            "Trainable params: 2,032,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_5 (Sequential)    (None, 327680)            626944    \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 1)                 9745      \n",
            "=================================================================\n",
            "Total params: 636,689\n",
            "Trainable params: 626,944\n",
            "Non-trainable params: 9,745\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-643f4c5abf56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;31m#  continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m   \u001b[0mbg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_bgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-643f4c5abf56>\u001b[0m in \u001b[0;36mshow_bgs\u001b[0;34m(test_case_dir)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_bgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_single_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_case_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_background_subtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a57f4c05308c>\u001b[0m in \u001b[0;36mget_single_test\u001b[0;34m(single_test_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprepend_10_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a57f4c05308c>\u001b[0m in \u001b[0;36mprepend_10_clips\u001b[0;34m(test_case)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepend_10_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtest_aug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWetO3yHoYee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(FPR_PLOT, TPR_PLOT, 'b', label = \"FPR vs TPR\")\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "print(\"TPR\")\n",
        "print(TPR_PLOT)\n",
        "print(\"FPR\")\n",
        "print(FPR_PLOT)\n",
        "\n",
        "def integrate(x, y):\n",
        "    area = np.trapz(y=y, x=x, dx=0.05)\n",
        "    return area\n",
        "\n",
        "\n",
        "print(\"AUC = \"+str(integrate(FPR_PLOT, TPR_PLOT)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}