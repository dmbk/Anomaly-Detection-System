{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST_AAE_ROC_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbk/Anomaly-Detection-System/blob/master/TEST_AAE_ROC_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVE526Rfl8ed",
        "colab_type": "code",
        "outputId": "baf5a883-efea-43d2-dd76-afea27b7bbc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install imageio\n",
        "!pip install qpsolvers\n",
        "!pip install shapely \n",
        "#!pip install tensorflow_datasets\n",
        "!pip install keras-layer-normalization\n",
        "from google.colab import drive\n",
        "#!pip install alive-progress\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drSah6JgmAU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import skimage\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "#import keras\n",
        "import argparse\n",
        "from os.path import dirname\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\t\n",
        "import statistics\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "#from progress.bar import IncrementalBar\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D, Dropout, Activation, InputLayer\n",
        "from keras.optimizers import Adam\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "\n",
        "from scipy.signal import savgol_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJ798qPmI9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_single_test(single_test_path):\n",
        "    \n",
        "    sz = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "          sz = sz +1\n",
        "    test = np.zeros(shape=(sz, conf.dim2, conf.dim3, conf.dim4))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "            #print(\"img path: \"+join(single_test_path, f))\n",
        "            img = Image.open(join(single_test_path, f)).resize((conf.dim2, conf.dim3))\n",
        "            #cv2_imshow(np.array(img,dtype=np.float32))\n",
        "            #cv2.waitKey(0)\n",
        "            img = np.array(img, dtype=np.float32) / 256\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    return test\n",
        "\n",
        "def get_test_sequences(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    print(\"Test case loaded\")\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgtzofEvwlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    def __init__(self, data_dir_, cwdir_name_, data_set):\n",
        "        self.data_set_name = data_set\n",
        "        self.data_dir = data_dir_\n",
        "        self.data_set_dir = join(self.data_dir, data_set)\n",
        "        self.cwdir_name = cwdir_name_\n",
        "        self.cwdir = join(self.data_dir,self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "        if not os.path.exists(self.cwdir):\n",
        "            os.mkdir(self.cwdir)\n",
        "            os.mkdir(self.run_data)\n",
        "    \n",
        "        if not os.path.exists(self.run_data):\n",
        "            #shutil.rmtree(self.run_data)\n",
        "            os.mkdir(self.run_data)\n",
        "            os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "        self.DATASET_PATH = join(self.data_set_dir,\"Train/\")\n",
        "        self.TEST_DIR = join(self.data_set_dir,\"Test/\")\n",
        "        self.BATCH_SIZE = 2\n",
        "        self.EPOCHS = 50\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = 0\n",
        "        self.dim1 = 10\n",
        "        self.dim2 = 256\n",
        "        self.dim3 = 256\n",
        "        self.dim4 = 1\n",
        "        self.latent_dim = 327680\n",
        "\n",
        "\n",
        "    def reconfig(self, new_name, batch_size = 4, epochs = 5, retrain = 0):\n",
        "        self.cwdir_name = new_name\n",
        "        self.cwdir = join(self.data_dir, self.cwdir_name)\n",
        "        self.run_data = join(self.cwdir, \"training_dir\")\n",
        "        self.image_dir = join(self.run_data,self.data_set_name,\"Test/\")\n",
        "\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.EPOCHS = epochs\n",
        "        self.GEN_MODEL_PATH = join(self.cwdir,\"model_gen_Conv2DLSTM_AAE\")\n",
        "        self.DIS_MODEL_PATH = join(self.cwdir,\"model_dis_Conv2DLSTM_AAE\")\n",
        "        self.DEC_MODEL_PATH = join(self.cwdir,\"model_dec_Conv2DLSTM_AAE\")\n",
        "\n",
        "        self.retrain = retrain\n",
        "        if retrain == 0:\n",
        "            print(\"Configuring train from scratch\")\n",
        "            if not os.path.exists(self.cwdir):\n",
        "                os.mkdir(self.cwdir)\n",
        "                os.mkdir(self.run_data)\n",
        "    \n",
        "            if os.path.exists(self.run_data):\n",
        "                shutil.rmtree(self.run_data)\n",
        "                os.mkdir(self.run_data)\n",
        "                os.makedirs(self.image_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "conf = Config(data_dir_=\"/content/drive/My Drive/\", cwdir_name_=\"Conv2DLSTM_AAE_PED1\", data_set=\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQZhjkL1NycS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '/content/drive/My Drive/Persistence1D/python')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from persistence1d import RunPersistence\n",
        "from reconstruct1d import RunReconstruction\n",
        "import math    \n",
        "\n",
        "from shapely.geometry import LineString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjBu8hyrfnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestVideoFile_ped1 = list((36,2))\n",
        "end = 190\n",
        "TestVideoFile_ped1 = [[60,152], [50,175], [91,end], [31,168], [5,90], [1,100], [1,175], [1,94], [1,48], [1,140],   [70,165],   [130,end],   [1,156],   [1,end],   [138,end],   [123,end],   [1,47],   [54,120],    [64,138],    [45,175],    [31,end],    [16,107],    [8,165],    [50,171],    [40,135],    [77,144],    [10,122],    [105,end],    [1,15],    [175,end],    [1,180],    [1,52],  [5,165],    [1,121],    [86,end],   [15,108]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def SortExtremaByPersistence(ExtremaAndPersistence):\n",
        "    #~ Sort the list of extrema by persistence.\n",
        "    #~ The original list from RunPersistence() is not guaranteed to be sorted,\n",
        "    #~ although it may appear sorted in many cases.\n",
        "    #~ This call to sorted() creates a new list. If you want to sort in-place, use ExtremaAndPersistence.sort()\n",
        "    SortedExtremaAndPersistence = sorted(ExtremaAndPersistence, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[0])\n",
        "    return SortedExtremaAndPersistence\n",
        "\n",
        "def GetMinima(ExtremaAndPersistence):\n",
        "    Minima = [t for t in ExtremaAndPersistence[::2]]\n",
        "    return Minima\n",
        "\n",
        "def GetMaxima(ExtremaAndPersistence):\n",
        "    Maxima = [t for t in ExtremaAndPersistence[1::2]]\n",
        "    return Maxima\n",
        "\n",
        "\n",
        "def GetIntersection(x, f, g):\n",
        "\n",
        "    plt.plot(x, f)\n",
        "    plt.plot(x, g)\n",
        "\n",
        "    first_line = LineString(np.column_stack((x, f)))\n",
        "    second_line = LineString(np.column_stack((x, g)))\n",
        "    intersection = first_line.intersection(second_line)\n",
        "    print(\"Intersection\")\n",
        "    #print(intersection)\n",
        "    x_list = []\n",
        "    if intersection.geom_type == 'MultiPoint':\n",
        "        plt.plot(*LineString(intersection).xy, 'o')\n",
        "        print(\"Multipoint\")\n",
        "        print(*LineString(intersection).xy[0])\n",
        "        for x_point in LineString(intersection).xy[0]:\n",
        "            x_list.append(x_point)\n",
        "    elif intersection.geom_type == 'Point':\n",
        "        plt.plot(*intersection.xy, 'o')\n",
        "        print(\"Point\")\n",
        "        print(*intersection.xy[0])\n",
        "        x_list.append(*intersection.xy[0])\n",
        "\n",
        "    x_arr = np.array(x_list)\n",
        "    #print(\"X_LIST\")\n",
        "    #print(x_arr)\n",
        "    return x_arr\n",
        "    #all(i >= 30 for i in g[])\n",
        "\n",
        "def find_overlap_rate(reference, detection):\n",
        "\n",
        "    overlap = 0\n",
        "    total_ref = 0\n",
        "    for x, y in detection:\n",
        "\n",
        "        print(\"Intersect x, y   =   \"+str(x)+\" , \"+str(y))\n",
        "        for p, q in reference:\n",
        "            print(\"Reference p, q   =   \"+str(p)+\" , \"+str(q))\n",
        "            if y > p and x < q:\n",
        "                overlap = overlap+ abs(min(y,q) - max(x,p))\n",
        "\n",
        "    for p, q in reference:\n",
        "        total_ref = total_ref+ abs(q-p)\n",
        "    \n",
        "    print(\"Total Ref: \"+str(total_ref))\n",
        "    print(\"Overlap: \"+str(overlap))\n",
        "    retVal = overlap\n",
        "    if retVal > 0 :\n",
        "        retVal = retVal/total_ref\n",
        "    return retVal\n",
        "\n",
        "def get_TPR_FPR(intersections, threshold_abs, gt_tuples, all_below_th):\n",
        "    print(\"_________Find TPR FPR for Threshold : \"+str(threshold_abs)+\"__________\")\n",
        "    print(\"============Find TPR============\")\n",
        "    TPR = find_overlap_rate(gt_tuples, all_below_th)\n",
        "    \n",
        "    gt_inverse_tuples = []\n",
        "\n",
        "    tf_start = 0\n",
        "    tf_abs_end = intersections[intersections.shape[0]-1]\n",
        "    for p, q in gt_tuples:\n",
        "        if(p == tf_start):\n",
        "            tf_start = q\n",
        "            continue\n",
        "        gt_inverse_tuples.append((tf_start, p))\n",
        "        tf_start = q\n",
        "\n",
        "    if tf_start < tf_abs_end:\n",
        "        gt_inverse_tuples.append((tf_start, tf_abs_end))\n",
        "    print(gt_inverse_tuples)\n",
        "    print(\"============Find FPR============\")\n",
        "    FPR = find_overlap_rate(gt_inverse_tuples, all_below_th)\n",
        "\n",
        "    return TPR, FPR\n",
        "\n",
        "def persistence(InputData, dt, id, color):\n",
        "    #~ Compute the extrema of the given data and their persistence.\n",
        "    ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "    #~ Keep only those extrema with a persistence larger than 0.5.\n",
        "    FilteredIndices = [t[0] for t in ExtremaAndPersistence if t[1] >= 0.01]\n",
        "\n",
        "    #~ This simple call is all you need to reconstruct a smooth function containing only the filtered extrema\n",
        "    SmoothData = RunReconstruction(InputData, FilteredIndices, 'biharmonic', 0.0000001)\n",
        "    #print(\"Smooth data \")\n",
        "    #print(SmoothData)\n",
        "\n",
        "    x = np.array([x for x in range(0, InputData.shape[0])])\n",
        "    \n",
        "    g = SmoothData\n",
        "\n",
        "\n",
        "    ##add gt_tuples : TODO\n",
        "    \n",
        "    #print(intersections)\n",
        "    #~ Plot original and smoothed data\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(0, len(InputData)), InputData, label=\"Original Data\")\n",
        "    ax.plot(range(0, len(SmoothData)), SmoothData, label=\"Smooth Data\")\n",
        "    ExtremaIndices = [t[0] for t in ExtremaAndPersistence]\n",
        "    \n",
        "    \"\"\"\n",
        "    sorted_extr = SortExtremaByPersistence(ExtremaAndPersistence)\n",
        "    minima = SortExtremaByPersistence(GetMinima(ExtremaAndPersistence))\n",
        "    maxima = SortExtremaByPersistence(GetMaxima(ExtremaAndPersistence))\n",
        "    print(\"###########Print All Extremas#################\")\n",
        "\n",
        "  \n",
        "    print([t for t in ExtremaAndPersistence])\n",
        "\n",
        "    print(\"###########Print Filtered Extremas#################\")\n",
        "\n",
        "    print([t for t in FilteredIndices])\n",
        "\n",
        "    print(\"###########Print Sorted Extremas#################\")\n",
        "\n",
        "    print([t for t in sorted_extr])\n",
        "\n",
        "    print(\"###########Print Minimas#################\")\n",
        "\n",
        "    print([t for t in minima])\n",
        "\n",
        "    print(\"###########Print Maximas#################\")\n",
        "\n",
        "    print([t for t in maxima])\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    gt_tuples = []\n",
        "    ax.plot(ExtremaIndices, InputData[ExtremaIndices], marker='.', linestyle='')\n",
        "    ax.plot(FilteredIndices, InputData[FilteredIndices], marker='*', linestyle='')\n",
        "    ax.set(xlabel='data index', ylabel='data value')\n",
        "    #ax.set_aspect(1.0/ax.get_data_ratio()*0.2)\n",
        "    plt.axvspan(TestVideoFile_ped1[id-1][0]-1, TestVideoFile_ped1[id-1][1]-1, alpha=0.5, color=color)\n",
        "    #plt.axhline(y=threshold_abs,linewidth=1, color='blue') \n",
        "\n",
        "\n",
        "    gt_tuples.append((TestVideoFile_ped1[id-1][0]-1, TestVideoFile_ped1[id-1][1]-1))\n",
        "    if dt == 1 and id == 5:\n",
        "        plt.axvspan(140-1, end-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((140-1, end-1))\n",
        "    elif dt == 1 and id == 6:\n",
        "        plt.axvspan(110-1, end-1, alpha=0.5, color=color) \n",
        "        gt_tuples.append((110-1, end-1))\n",
        "    elif dt == 1 and id == 29:\n",
        "        plt.axvspan(45-1, 113-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((45-1, 113-1))\n",
        "    elif dt == 1 and id == 32:\n",
        "        plt.axvspan(65-1, 115-1, alpha=0.5, color=color)\n",
        "        gt_tuples.append((65-1, 115-1))\n",
        "\n",
        "\n",
        "    TPR_ARR = []\n",
        "    FPR_ARR = []\n",
        "\n",
        "    for threshold_abs in np.arange(0.6, 1.05, 0.05):\n",
        "        f = np.ones(x.shape[0])*threshold_abs\n",
        "        intersections = [0]\n",
        "        intersections.extend(GetIntersection(x, f, g))\n",
        "        if(intersections[len(intersections)-1] < end -1):\n",
        "            intersections.extend([end-1])\n",
        "\n",
        "        intersections = np.array(intersections)\n",
        "        all_below_th = []\n",
        "        #all_above_th = []\n",
        "\n",
        "        for j, __ in enumerate(intersections):\n",
        "\n",
        "            if all(i <= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "                all_below_th.append((intersections[j], intersections[j+1]))\n",
        "                \n",
        "            if(end - 1 == math.floor(intersections[j+1])):\n",
        "                break\n",
        "            #if all(i >= threshold_abs for i in g[math.ceil(intersections[j]): math.floor(intersections[j+1])]):\n",
        "            #    all_above_th.append((intersections[j], intersections[j+1]))\n",
        "\n",
        "\n",
        "        TPR, FPR = get_TPR_FPR(intersections, threshold_abs, gt_tuples, all_below_th)\n",
        "        \n",
        "        TPR_ARR.append(TPR)\n",
        "        FPR_ARR.append(FPR)\n",
        "    \n",
        "        print(\"TPR : \"+str(TPR))\n",
        "        print(\"FPR : \"+str(FPR))\n",
        "        print(\"\\n\\n\\n\")\n",
        "    \n",
        "    ax.grid()\n",
        "    #ax.plot()\n",
        "\n",
        "    plt.legend()    \n",
        "    plt.show()\n",
        "\n",
        "    return TPR_ARR, FPR_ARR\n",
        "\n",
        "threshold = 0.9\n",
        "def fill_gt_ped1(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    print(sr.shape)\n",
        "    #zeros = np.zeros((conf.dim1,))\n",
        "    #print(zeros.shape)\n",
        "    #plt.plot(np.concatenate((zeros, sr)))\n",
        "    #persistence(sr, 1, id, color)\n",
        "    yhat = savgol_filter(sr, 51, 3)\n",
        "    TPR_ARR, FPR_ARR = persistence(yhat, 1, id, color)\n",
        "    #plt.plot(sr)  \n",
        "    return TPR_ARR, FPR_ARR\n",
        "\n",
        "    #plt.plot(yhat, color='red')\n",
        "    plt.axvspan(TestVideoFile_ped1[id-1][0], TestVideoFile_ped1[id-1][1], alpha=0.5, color=color)\n",
        "    #plt.axhline(y=threshold_abs,linewidth=1, color='blue')\n",
        "    if id == 5:\n",
        "        plt.axvspan(140, 200, alpha=0.5, color=color)\n",
        "    elif id == 6:\n",
        "        plt.axvspan(110, 200, alpha=0.5, color=color) \n",
        "    elif id == 29:\n",
        "        plt.axvspan(45, 113, alpha=0.5, color=color)\n",
        "    elif id == 32:\n",
        "        plt.axvspan(65, 115, alpha=0.5, color=color)\n",
        "\n",
        "\n",
        "    plt.ylabel('regularity score sr_ae(t)')\n",
        "    plt.xlabel('frame t')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "TestVideoFile_ped2 = list((12,2))\n",
        "TestVideoFile_ped2 = [[61,180],[95,180],[1,146],[31,180],[1,129],[1,159],[46,180],[1,180],[1,120],[1,150],[1,180],[88,180]]\n",
        "\n",
        "def fill_gt_ped2(sr, id, color):\n",
        "    #plt.xlim((conf.dim1,len(sr)+conf.dim1))\n",
        "    sr = np.reshape(sr, (sr.shape[0],))\n",
        "    #plt.plot(np.concatenate((np.zeros((conf.dim1,)), sr)))\n",
        "    persistence(sr, 2, id, color)\n",
        "    yhat = savgol_filter(sr, 51, 3)\n",
        "    TPR_ARR, FPR_ARR = persistence(yhat, 2, id, color)\n",
        "    return TPR_ARR, FPR_ARR\n",
        "\n",
        "    #plt.plot(sr)\n",
        "\n",
        "    #plt.plot(yhat, color='red')\n",
        "    plt.axvspan(TestVideoFile_ped2[id-1][0], TestVideoFile_ped2[id-1][1], alpha=0.5, color=color)\n",
        "    #plt.axhline(y=threshold_abs,linewidth=1, color='blue')\n",
        "    plt.ylabel('regularity score sr_ae(t)')\n",
        "    plt.xlabel('frame t')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXfjsTem0oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_aae():\n",
        "  \n",
        "    model_enc = load_model(conf.cwdir+\"/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_dec = load_model(conf.cwdir+\"/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    model_disc = load_model(conf.cwdir+\"/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    #model_enc = load_model(\"/content/drive/My Drive/model_gen_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_dec = load_model(\"/content/drive/My Drive/model_dec_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #model_disc = load_model(\"/content/drive/My Drive/model_dis_Conv2DLSTM_AAEep100\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "    model_ae = Sequential()\n",
        "    model_ae.add(model_enc)\n",
        "    model_ae.add(model_dec)\n",
        "    \n",
        "    model_enc_disc = Sequential()\n",
        "    model_enc_disc.add(model_enc)\n",
        "    model_enc_disc.add(model_disc)\n",
        "    \n",
        "    return model_enc, model_dec, model_disc, model_ae, model_enc_disc\n",
        "\n",
        "model_enc, model_dec, model_disc, model_ae, model_enc_disc = build_model_aae()\n",
        "\n",
        "model_enc.summary()\n",
        "model_dec.summary()\n",
        "model_disc.summary()\n",
        "model_ae.summary()\n",
        "model_enc_disc.summary()\n",
        "\n",
        "model_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_enc_disc.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\")\n",
        "model_ae.compile(optimizer=Adam(lr=1e-4, decay=1e-5, epsilon=1e-6), loss=\"mse\")\n",
        "#\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/model.hdf5\"\n",
        "\n",
        "#model_ae = load_model(\"/content/drive/My Drive/VAE/model1.hdf5\",custom_objects={'LayerNormalization': LayerNormalization})\n",
        "\n",
        "def evaluate_dis(sequences, model, id, dt):\n",
        "    fooling_loss = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    sa = (fooling_loss - np.min(fooling_loss)) / (np.max(fooling_loss))\n",
        "    sr = 1.0 - sa\n",
        "\n",
        "    with open(join(\"/content/drive/My Drive/\", 'sr_score.csv'), mode='a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([float(x[0]) for x in sr])\n",
        "        f.close()\n",
        "\n",
        "    threshold_abs = threshold# np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        fill_gt_ped1(sr, id, 'red', threshold_abs)\n",
        "    elif dt == 2:\n",
        "        fill_gt_ped2(sr, id, 'red', threshold_abs)\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_dis(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "\n",
        "def get_clips(test):\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :]\n",
        "        sequences[i] = clip\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def mask_array(bg_list):\n",
        "\n",
        "    for p in range(0, bg_list.shape[0]):\n",
        "        for r in range(256):\n",
        "            for c in range(256):\n",
        "                if bg_list[p][r][c] > 0:\n",
        "                    bg_list[p][r][c] = 1\n",
        "                else:\n",
        "                    bg_list[p][r][c] = 0\n",
        "\n",
        "    bg_list[0, :, :] = 0\n",
        "    return bg_list\n",
        "\n",
        "def evaluate_ae(sequences, model, id, dt, bg_list=[], mask=1):\n",
        "  \n",
        "    sz = len(sequences)\n",
        "\n",
        "    reconstructed_sequences = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    #print(bg_list[10])\n",
        "\n",
        "    masked_bg = mask_array(bg_list)\n",
        "    bg_clips = get_clips(masked_bg)\n",
        "\n",
        "    sequences = np.reshape(sequences, (sequences.shape[0], sequences.shape[1], sequences.shape[2], sequences.shape[3]))\n",
        "    reconstructed_sequences = np.reshape(reconstructed_sequences, (reconstructed_sequences.shape[0], reconstructed_sequences.shape[1], reconstructed_sequences.shape[2], reconstructed_sequences.shape[3]))\n",
        "\n",
        "    if mask == 1:\n",
        "        print(\"==============With BG Masked==============\")\n",
        "        sq_masked = np.multiply(sequences, bg_clips)\n",
        "        rec_masked = np.multiply(reconstructed_sequences, bg_clips)\n",
        "    else:\n",
        "        print(\"==============Without BG Masked==============\")\n",
        "        sq_masked = sequences\n",
        "        rec_masked = reconstructed_sequences\n",
        "\n",
        "    #bg_weights = np.add(np.ones(bg_clips.shape), bg_clips)\n",
        "    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sq_masked[i], rec_masked[i])) for i in range(0,sz)])\n",
        "    #sequences_reconstruction_cost = np.array([np.linalg.norm(np.multiply(bg_weights[i],np.subtract(sequences[i], reconstructed_sequences[i]))) for i in range(0,sz)])\n",
        "\n",
        "\n",
        "    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / (np.max(sequences_reconstruction_cost))\n",
        "\n",
        "    \n",
        "    sr = 1 - sa\n",
        "    #threshold_abs = threshold#np.min(sr)+ (np.max(sr) - np.min(sr)) * threshold\n",
        "    if dt == 1:\n",
        "        TPR_ARR, FPR_ARR = fill_gt_ped1(sr, id, 'yellow')\n",
        "    elif dt == 2:\n",
        "        TPR_ARR, FPR_ARR = fill_gt_ped2(sr, id, 'yellow')\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score sr_ae(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "    return TPR_ARR, FPR_ARR\n",
        "\n",
        "def load_input_data_list():\n",
        "    path = join(\"/content/drive/My Drive/\", 'sr_score.csv')\n",
        "    InputDataList = LoadData(path)\n",
        "    return InputDataList\n",
        "\n",
        "\n",
        "def get_persistance(InputData):\n",
        "  #~ This simple call is all you need to compute the extrema of the given data and their persistence.\n",
        "  ExtremaAndPersistence = RunPersistence(InputData)\n",
        "\n",
        "  #~ Keep only those extrema with a persistence larger than 10.\n",
        "  Filtered = [t for t in ExtremaAndPersistence if ExtremaAndPersistence[1] > 50]\n",
        "  print(ExtremaAndPersistence)\n",
        "  print(Filtered)\n",
        "  #~ Sort the list of extrema by persistence.\n",
        "  #Sorted = sorted(Filtered, key=lambda ExtremumAndPersistence: ExtremumAndPersistence[1])\n",
        "\n",
        "  return Filtered\n",
        "\n",
        "\n",
        "conf.reconfig(new_name=\"Conv2DLSTM_AAE_PED1\", batch_size=4, epochs=100, retrain=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def edge_detect(gray):\n",
        "    edges1 = np.uint8(gray)\n",
        "    # Using the Canny filter to get contours\n",
        "    #edges = cv2.Canny(edges1, 20, 30)\n",
        "    \n",
        "    # Using the Canny filter with different parameters\n",
        "    edges_high_thresh = cv2.Canny(edges1, 160, 200)\n",
        "    kernel = np.zeros((5,5),np.uint8)\n",
        "    kernel2 = np.zeros((2,2),np.uint8)\n",
        "\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(edges_high_thresh, cv2.MORPH_OPEN, kernel)\n",
        "    images = cv2.morphologyEx(images, cv2.MORPH_CLOSE, kernel2)\n",
        "    # Stacking the images to print them together\n",
        "    # For comparison\n",
        "    #images = np.hstack((gray, edges, edges_high_thresh))\n",
        "\n",
        "    #kernel = np.ones((3,3), np.uint8) \n",
        "    #fg_mask = cv2.erode(edges_high_thresh, kernel, iterations=2)\n",
        "    # Display the resulting frame\n",
        "    #cv2_imshow(images)\n",
        "    return images\n",
        "\n",
        "    \n",
        "\n",
        "def connected_comp(img):\n",
        "\n",
        "    #find all your connected components (white blobs in your image)\n",
        "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
        "    #connectedComponentswithStats yields every seperated component with information on each of them, such as size\n",
        "    #the following part is just taking out the background which is also considered a component, but most of the time we don't want that.\n",
        "    sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
        "\n",
        "    # minimum size of particles we want to keep (number of pixels)\n",
        "    #here, it's a fixed value, but you can set it as you want, eg the mean of the sizes or whatever\n",
        "    min_size = 10  \n",
        "\n",
        "    #your answer image\n",
        "    img2 = np.zeros((output.shape))\n",
        "    #for every component in the image, you keep it only if it's above min_size\n",
        "    for i in range(0, nb_components):\n",
        "        if sizes[i] >= min_size:\n",
        "            img2[output == i + 1] = 255\n",
        "    return img2\n",
        "\n",
        "def get_background_subtr(test_case):\n",
        "    print(\"BGS show\")\n",
        "    #subtractor = cv2.createBackgroundSubtractorMOG2(history=30, varThreshold=50, detectShadows=True)\n",
        "    bg_list = []\n",
        "    for frame in test_case:\n",
        "        frame = np.reshape(frame, (256, 256))*256\n",
        "        edge_mask = edge_detect(frame)\n",
        "        bg_mask = connected_comp(edge_mask)\n",
        "        #bg_mask = subtractor.apply(edge_mask)\n",
        "        #edge_mask[0] = np.zeros(edge_mask[0].shape)\n",
        "        #kernel = np.ones((5,5), np.uint8) \n",
        "        #fg_mask = cv2.erode(mask, kernel, iterations=2)\n",
        "        #fg_mask = cv2.dilate(fg_mask, kernel, iterations=5)\n",
        "        fg_mask = bg_mask/255.0\n",
        "        #fg_mask = mask\n",
        "        #cv2_imshow(fg_mask)\n",
        "        #print(fg_mask)\n",
        "        bg_list.append(fg_mask)\n",
        "        #key = cv2.waitKey(30)\n",
        "        #if key == 27:\n",
        "        #    break\n",
        "    cv2_imshow(bg_list[100]*255.0)\n",
        "    #cv2.destroyAllWindows()\n",
        "    #print(bg_list)\n",
        "    return np.array(bg_list)\n",
        "\n",
        "\n",
        "def show_bgs(test_case_dir):\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir))\n",
        "    return get_background_subtr(test)\n",
        "\n",
        "\n",
        "TPR_FOR_CASES = []\n",
        "FPR_FOR_CASES = []\n",
        "#print(bg_list[10])\n",
        "for i in range(1,37):\n",
        "  if i < 10:\n",
        "    img_num = \"00\"+str(i)\n",
        "  elif i < 100:\n",
        "    img_num = \"0\"+str(i)\n",
        "  else:\n",
        "    img_num = str(i) \n",
        "\n",
        "  #if img_num == \"017\":\n",
        "  #  continue\n",
        "\n",
        "  bg_list = show_bgs(\"Test\"+img_num)\n",
        "  \n",
        "  \n",
        "  test_cases_dir = \"Test\"+img_num\n",
        "  test_cases = get_test_sequences(test_cases_dir)\n",
        "  print(\"Test\"+img_num+\" data set loaded\")\n",
        "  #evaluate_dis(test_cases, model_enc_disc, i, 1)\n",
        "  #sorted_sr = get_persistance(np.array([x[0] for x in sr_dis], dtype=float))\n",
        "  #print(sorted_sr)\n",
        "  #break\n",
        "  TPR_ARR, FPR_ARR = evaluate_ae(test_cases, model_ae, i, 1, bg_list, 1)\n",
        "  TPR_FOR_CASES.append(TPR_ARR)\n",
        "  FPR_FOR_CASES.append(FPR_ARR)\n",
        "  #evaluate_ae(test_cases, model_ae, i, 1, bg_list, 0)\n",
        "\n",
        "  #sr_comb = (sr_dis + sr_ae)*0.5\n",
        "  #sr_comb = []\n",
        "\n",
        "  continue\n",
        "TPR_FOR_CASES = numpy.array(TPR_FOR_CASES)\n",
        "FPR_FOR_CASES = numpy.array(FPR_FOR_CASES)\n",
        "\n",
        "TPR_PLOT = numpy.mean(TPR_FOR_CASES, axis=0)\n",
        "FPR_PLOT = numpy.mean(FPR_FOR_CASES, axis=0)\n",
        "\n",
        "print(TPR_PLOT.shape)\n",
        "print(FPR_PLOT.shape)\n",
        "\n",
        "plt.plot(TPR_PLOT)\n",
        "plt.plot(FPR_PLOT)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}