{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anomaly_detection_gist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/9kOdTMfdD/y+Wu+vk35b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbk/Anomaly-Detection-System/blob/master/anomaly_detection_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uQhe8VSqXpe",
        "colab_type": "code",
        "outputId": "f6f2b121-2da1-4cd3-b2ef-f7db4d13ae77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pip install keras-layer-normalization"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-layer-normalization in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (1.18.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn4T13Cd2Vwh",
        "colab_type": "code",
        "outputId": "bd97a5fe-9de1-499a-8f59-34e9dd694c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbKe11RPoo9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import skimage\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D\n",
        "from keras.models import Sequential, load_model\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "from keras.layers import Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras import backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import numpy as np\n",
        "from os.path import dirname\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "class Config:\n",
        "    def __init__(self, data_dir, single_test_case, model_path, retrain=0, reload_weights=0, curr_epoch=0):\n",
        "        self.DATASET_PATH = join(data_dir,\"UCSDped1/Train\")\n",
        "        self.TEST_DIR = join(data_dir,\"UCSDped1/Test/\")\n",
        "        self.SINGLE_TEST_PATH = join(self.TEST_DIR, single_test_case)\n",
        "        self.BATCH_SIZE = 4\n",
        "        self.EPOCHS = 50\n",
        "        self.MODEL_PATH = join(model_path,\"model\")\n",
        "        self.retrain = retrain\n",
        "        self.latent_dim = 128\n",
        "        self.reload_weights = reload_weights\n",
        "        self.curr_epoch = curr_epoch\n",
        "\n",
        "def get_clips_by_stride(stride, frames_list, sequence_size):\n",
        "    \"\"\" For data augmenting purposes.\n",
        "    Parameters\n",
        "    ----------\n",
        "    stride : int\n",
        "        The distance between two consecutive frames\n",
        "    frames_list : list\n",
        "        A list of sorted frames of shape 256 X 256\n",
        "    sequence_size: int\n",
        "        The size of the lstm sequence\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of clips , 10 frames each\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    sz = len(frames_list)\n",
        "    clip = np.zeros(shape=(sequence_size, 256, 256, 1))\n",
        "    cnt = 0\n",
        "    for start in range(0, stride):\n",
        "        for i in range(start, sz, stride):\n",
        "            clip[cnt, :, :, 0] = frames_list[i]\n",
        "            cnt = cnt + 1\n",
        "            if cnt == sequence_size:\n",
        "                clips.append(clip)\n",
        "                cnt = 0\n",
        "    return clips\n",
        "\n",
        "def get_training_set(conf):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of training sequences of shape (NUMBER_OF_SEQUENCES,SINGLE_SEQUENCE_SIZE,FRAME_WIDTH,FRAME_HEIGHT,1)\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    # loop over the training folders (Train000,Train001,..)\n",
        "    for f in sorted(listdir(conf.DATASET_PATH)):\n",
        "        directory_path = join(conf.DATASET_PATH, f)\n",
        "        if isdir(directory_path):\n",
        "            all_frames = []\n",
        "            # loop over all the images in the folder (0.tif,1.tif,..,199.tif)\n",
        "            for c in sorted(listdir(directory_path)):\n",
        "                img_path = join(directory_path, c)\n",
        "                if str(img_path)[-3:] == \"tif\":\n",
        "                    img = Image.open(img_path).resize((256, 256))\n",
        "\n",
        "                    img = np.array(img, dtype=np.float32) / 256.0\n",
        "                    all_frames.append(img)\n",
        "            # get the 10-frames sequences from the list of images after applying data augmentation\n",
        "            for stride in range(1, 3):\n",
        "                clips.extend(get_clips_by_stride(stride=stride, frames_list=all_frames, sequence_size=10))\n",
        "    return clips\n",
        "\n",
        "\n",
        "\n",
        "def get_model(conf):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    reload_model : bool\n",
        "        Load saved model or retrain it\n",
        "    \"\"\"\n",
        "\n",
        "    if conf.retrain == 1:\n",
        "        input_train = get_training_set(conf)\n",
        "        input_train = np.array(input_train)\n",
        "\n",
        "    validation_split = 0.2\n",
        "    verbosity = 1\n",
        "\n",
        "    # # =================\n",
        "    # # Encoder\n",
        "    # # =================\n",
        "    \n",
        "    # Definition\n",
        "    i       = Input(shape=(10, 256, 256, 1), name='encoder_input')\n",
        "    cx      = TimeDistributed(Conv2D(128, (11, 11), strides=4, padding=\"same\", activation='relu'))(i)\n",
        "    cx      = LayerNormalization()(cx)\n",
        "    cx      = TimeDistributed(Conv2D(64, (5, 5), strides=2, padding=\"same\", activation='relu'))(cx)\n",
        "    cx      = LayerNormalization()(cx)\n",
        "    cx      = ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True)(cx)\n",
        "    cx      = LayerNormalization()(cx)\n",
        "    cx      = ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True)(cx)\n",
        "    x       = Flatten()(cx)\n",
        "    x       = Dense(360, activation='relu')(x)\n",
        "    x       = BatchNormalization()(x)\n",
        "    mu      = Dense(conf.latent_dim, name='latent_mu')(x)\n",
        "    sigma   = Dense(conf.latent_dim, name='latent_sigma')(x)\n",
        "    \n",
        "    # Get Conv2D shape for Conv2DTranspose operation in decoder\n",
        "    conv_shape = K.int_shape(cx)\n",
        "    \n",
        "    # Define sampling with reparameterization trick\n",
        "    def sample_z(args):\n",
        "      mu, sigma = args\n",
        "      batch     = K.shape(mu)[0]\n",
        "      dim       = K.int_shape(mu)[1]\n",
        "      eps       = K.random_normal(shape=(batch, dim))\n",
        "      return mu + K.exp(sigma / 2) * eps\n",
        "    \n",
        "    # Use reparameterization trick to ....??\n",
        "    z       = Lambda(sample_z, output_shape=(conf.latent_dim, ), name='z')([mu, sigma])\n",
        "    \n",
        "    def kl_reconstruction_loss(true, pred):\n",
        "    # Reconstruction loss\n",
        "      #reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * 10 * 256 * 256\n",
        "      #reconstruction_loss = np.linalg.norm(np.subtract(true,pred))\\\n",
        "\n",
        "        reconstruction_loss = mse(K.flatten(true), K.flatten(pred)) * 10 * 256 * 256\n",
        "      #reconstruction_loss = euclidean_distance_loss(K.flatten(true), K.flatten(pred))\n",
        "      # KL divergence loss\n",
        "        kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "      # Total loss = 50% rec + 50% KL divergence loss\n",
        "        return K.mean(reconstruction_loss + kl_loss)\n",
        "    # Instantiate encoder\n",
        "    encoder = Model(i, [mu, sigma, z], name='encoder')\n",
        "    encoder.summary()\n",
        "    \n",
        "    # =================\n",
        "    # Decoder\n",
        "    # =================\n",
        "    \n",
        "    # Definition\n",
        "    d_i   = Input(shape=(conf.latent_dim, ), name='decoder_input')\n",
        "    x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3]* conv_shape[4], activation='relu')(d_i)\n",
        "    x     = BatchNormalization()(x)\n",
        "    x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3], conv_shape[4]))(x)\n",
        "    cx    = ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True)(x)\n",
        "    cx    = LayerNormalization()(cx)\n",
        "    cx    = ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True)(cx)\n",
        "    cx    = LayerNormalization()(cx)\n",
        "    cx    = TimeDistributed(Conv2DTranspose(64, (5, 5), strides=2, padding=\"same\"))(cx)\n",
        "    cx    = LayerNormalization()(cx)\n",
        "    cx    = TimeDistributed(Conv2DTranspose(128, (11, 11), strides=4, padding=\"same\"))(cx)\n",
        "    o     = TimeDistributed(Conv2D(1, (11, 11), activation=\"sigmoid\", padding=\"same\"), name='decoder_output')(cx)\n",
        "\n",
        "    # Instantiate decoder\n",
        "    decoder = Model(d_i, o, name='decoder')\n",
        "    decoder.summary()\n",
        "    \n",
        "    # =================\n",
        "    # VAE as a whole\n",
        "    # =================\n",
        "    \n",
        "    # Instantiate VAE\n",
        "    vae_outputs = decoder(encoder(i)[2])\n",
        "    vae         = Model(i, vae_outputs, name='vae')\n",
        "    vae.summary()\n",
        "\n",
        "    def euclidean_distance_loss(y_true, y_pred):\n",
        "\n",
        "        return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
        "    # Define loss\n",
        "\n",
        "    \n",
        "    print(vae.summary())\n",
        "    # Compile VAE\n",
        "    vae.compile(optimizer='adam', loss=kl_reconstruction_loss)\n",
        "    !mkdir training_1\n",
        "    checkpoint_path = \"training_1/cp.ckpt\"\n",
        "    checkpoint_dir = dirname(checkpoint_path)\n",
        "\n",
        "    # Create a callback that saves the model's weights\n",
        "    cp_callback = ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)\n",
        "\n",
        "    # Train autoencoder\n",
        "    if conf.reload_weights == 1 and os.path.isfile(conf.MODEL_PATH+str(conf.curr_epoch)+\".hdf5\"):\n",
        "        vae.load_weights(conf.MODEL_PATH+str(conf.curr_epoch)+\".hdf5\")\n",
        "    else:\n",
        "        conf.curr_epoch = 0\n",
        "        \n",
        "    if conf.retrain == 0:\n",
        "        return vae\n",
        "    for i in range(conf.curr_epoch, conf.EPOCHS):\n",
        "        print(\"Starting epoch: \"+str(i+1))\n",
        "        vae.fit(input_train, input_train, epochs = 1, batch_size = conf.BATCH_SIZE, shuffle=False, callbacks=[cp_callback])\n",
        "        vae.save_weights(conf.MODEL_PATH+str(i+1)+\".hdf5\")\n",
        "    return vae\n",
        "\n",
        "\n",
        "def TEST_get_training_set(conf):\n",
        "    print(\"Starting\")\n",
        "    clips =  get_training_set(conf)\n",
        "    print(\"Total clips: %d\" % len(clips))\n",
        "    for clip in clips:\n",
        "        print(\"Sequence size: %d\" % len(clip))\n",
        "        for image in clip:\n",
        "            plt.imshow(np.uint8(image))\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def get_single_test(conf):\n",
        "\n",
        "    sz = 0\n",
        "    for f in sorted(listdir(conf.SINGLE_TEST_PATH)):\n",
        "        if str(join(conf.SINGLE_TEST_PATH, f))[-3:] == \"tif\":\n",
        "          sz = sz +1\n",
        "    test = np.zeros(shape=(sz, 256, 256, 1))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(conf.SINGLE_TEST_PATH)):\n",
        "        if str(join(conf.SINGLE_TEST_PATH, f))[-3:] == \"tif\":\n",
        "            img = Image.open(join(conf.SINGLE_TEST_PATH, f)).resize((256, 256))\n",
        "            img = np.array(img, dtype=np.float32) / 256.0\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    return test\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(conf):\n",
        "    model = get_model(conf)\n",
        "    print(\"got model\")\n",
        "    test = get_single_test(conf)\n",
        "    print(\"got test\")\n",
        "    sz = test.shape[0] - 10\n",
        "    sequences = np.zeros((sz, 10, 256, 256, 1))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((10, 256, 256, 1))\n",
        "        for j in range(0, 10):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "\n",
        "    # get the reconstruction cost of all the sequences\n",
        "    reconstructed_sequences = model.predict(sequences,batch_size=4)\n",
        "    for j in range(0, sz):\n",
        "        clip_10 = reconstructed_sequences[j]\n",
        "       #img_prev = np.reshape(clip_10[0],(256,256))*256\n",
        "        for k in range(1,2):\n",
        "            img = np.reshape(clip_10[k],(256,256))*256\n",
        "            #optical_flow(img, img_prev)\n",
        "\n",
        "    #        img_prev = img\n",
        "            #print(img)\n",
        "            #print(\"end of array\\n\")\n",
        "            cv2_imshow(img)\n",
        "            cv2.waitKey(0)\n",
        "\n",
        "    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sequences[i],reconstructed_sequences[i])) for i in range(0,sz)])\n",
        "    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)\n",
        "    sr = 1.0 - sa\n",
        "    plt.plot(sr)\n",
        "    plt.ylabel('regularity score Sr(t)')\n",
        "    plt.xlabel('frame t')\n",
        "    plt.show()\n",
        "    #sr = sigmoid(sr)\n",
        "    # plot the regularity scores\n",
        "    #plt.plot(sr)\n",
        "    #plt.ylabel('regularity score Sr(t)')\n",
        "    #plt.xlabel('frame t')\n",
        "    #plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcxkk6UpbgL",
        "colab_type": "code",
        "outputId": "2386d687-0727-44f1-d357-492604b87a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# Instantiate the parser\n",
        "#parser = argparse.ArgumentParser(description='Vidasa Extended -- Anomaly Detection System')\n",
        "\n",
        "\n",
        "#parser.add_argument('--data',type=str,\n",
        " #                   help='Data directory path')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "model_path = \"/content/drive/My Drive/VAE/\"\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "conf = Config(data_dir=\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/\", single_test_case=\"Test022\", model_path=model_path, retrain=1, reload_weights=0, curr_epoch=0)\n",
        "evaluate(conf)\n",
        "#TEST_get_training_set(conf)\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, 10, 256, 256, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_13 (TimeDistri (None, 10, 64, 64, 1 15616       encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_19 (LayerNo (None, 10, 64, 64, 1 256         time_distributed_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_14 (TimeDistri (None, 10, 32, 32, 6 204864      layer_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_20 (LayerNo (None, 10, 32, 32, 6 128         time_distributed_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_13 (ConvLSTM2D)    (None, 10, 32, 32, 6 295168      layer_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_21 (LayerNo (None, 10, 32, 32, 6 128         conv_lst_m2d_13[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_14 (ConvLSTM2D)    (None, 10, 32, 32, 3 110720      layer_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 327680)       0           conv_lst_m2d_14[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 360)          117965160   flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 360)          1440        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "latent_mu (Dense)               (None, 128)          46208       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "latent_sigma (Dense)            (None, 128)          46208       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 128)          0           latent_mu[0][0]                  \n",
            "                                                                 latent_sigma[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 118,685,896\n",
            "Trainable params: 118,685,176\n",
            "Non-trainable params: 720\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 327680)            42270720  \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 327680)            1310720   \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 10, 32, 32, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_15 (ConvLSTM2D) (None, 10, 32, 32, 32)    73856     \n",
            "_________________________________________________________________\n",
            "layer_normalization_22 (Laye (None, 10, 32, 32, 32)    64        \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_16 (ConvLSTM2D) (None, 10, 32, 32, 64)    221440    \n",
            "_________________________________________________________________\n",
            "layer_normalization_23 (Laye (None, 10, 32, 32, 64)    128       \n",
            "_________________________________________________________________\n",
            "time_distributed_15 (TimeDis (None, 10, 64, 64, 64)    102464    \n",
            "_________________________________________________________________\n",
            "layer_normalization_24 (Laye (None, 10, 64, 64, 64)    128       \n",
            "_________________________________________________________________\n",
            "time_distributed_16 (TimeDis (None, 10, 256, 256, 128) 991360    \n",
            "_________________________________________________________________\n",
            "decoder_output (TimeDistribu (None, 10, 256, 256, 1)   15489     \n",
            "=================================================================\n",
            "Total params: 44,986,369\n",
            "Trainable params: 44,331,009\n",
            "Non-trainable params: 655,360\n",
            "_________________________________________________________________\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 10, 256, 256, 1)   0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 128), (None, 128) 118685896 \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 10, 256, 256, 1)   44986369  \n",
            "=================================================================\n",
            "Total params: 163,672,265\n",
            "Trainable params: 163,016,185\n",
            "Non-trainable params: 656,080\n",
            "_________________________________________________________________\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 10, 256, 256, 1)   0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              [(None, 128), (None, 128) 118685896 \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 10, 256, 256, 1)   44986369  \n",
            "=================================================================\n",
            "Total params: 163,672,265\n",
            "Trainable params: 163,016,185\n",
            "Non-trainable params: 656,080\n",
            "_________________________________________________________________\n",
            "None\n",
            "mkdir: cannot create directory ‘training_1’: File exists\n",
            "Starting epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-778af75c71f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_test_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test022\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#TEST_get_training_set(conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cb6ac8c4ebdc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"got model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_single_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cb6ac8c4ebdc>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'input_train' referenced before assignment"
          ]
        }
      ]
    }
  ]
}