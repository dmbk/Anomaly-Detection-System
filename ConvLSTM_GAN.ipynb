{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvLSTM_GAN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOVPE77+QZNtPz42+gZrnxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbk/Anomaly-Detection-System/blob/master/ConvLSTM_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeDOtMkwIqL0",
        "colab_type": "code",
        "outputId": "ad3b1f59-549c-4f47-d0f8-8057c3290740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!pip install imageio\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.17.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (6.2.2)\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1qK4WayIRWG",
        "colab_type": "code",
        "outputId": "53274c3b-1893-483b-acba-62a4a19e45dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "#import keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Reshape, LeakyReLU, Conv2DTranspose, Conv3DTranspose, ConvLSTM2D, BatchNormalization, LayerNormalization, TimeDistributed, Conv2D, Conv3D, ZeroPadding3D, MaxPooling2D, MaxPooling3D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from os.path import dirname\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "tf.keras.backend.set_floatx('float32')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27gwZwBqIxav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    def __init__(self, data_dir):\n",
        "        self.DATASET_PATH = join(data_dir,\"UCSDped1/Train/\")\n",
        "        self.TEST_DIR = join(data_dir,\"UCSDped1/Test/\")\n",
        "        self.BATCH_SIZE = 2\n",
        "        self.EPOCHS = 25\n",
        "        self.GEN_MODEL_PATH = join(data_dir,\"model_gen_ConvLSTM_GAN.hdf5\")\n",
        "        self.DIS_MODEL_PATH = join(data_dir,\"model_dis_ConvLSTM_GAN.hdf5\")\n",
        "        self.GAN_MODEL_PATH = join(data_dir,\"model_combined_ConvLSTM_GAN.hdf5\")\n",
        "        self.dim1 = 10\n",
        "        self.dim2 = 256\n",
        "        self.dim3 = 256\n",
        "        self.dim4 = 1\n",
        "        self.r_alpha = 0.00000000000001\n",
        "\n",
        "conf = Config(data_dir=\"/content/drive/My Drive/UCSD_Anomaly_Dataset.v1p2/\") \n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "try: \n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
        "except: \n",
        "  # Invalid device or cannot modify virtual devices once initialized. \n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyRNft5JKae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clips_by_stride(stride, frames_list, sequence_size):\n",
        "    \"\"\" For data augmenting purposes.\n",
        "    Parameters\n",
        "    ----------\n",
        "    stride : int\n",
        "        The distance between two consecutive frames\n",
        "    frames_list : list\n",
        "        A list of sorted frames of shape 256 X 256\n",
        "    sequence_size: int\n",
        "        The size of the lstm sequence\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of clips , 32 frames each\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    sz = len(frames_list)\n",
        "    clip = np.zeros(shape=(sequence_size, 256, 256, 1))\n",
        "    cnt = 0\n",
        "    for start in range(0, stride):\n",
        "        for i in range(start, sz, stride):\n",
        "            clip[cnt, :, :, 0] = frames_list[i]\n",
        "            cnt = cnt + 1\n",
        "            if cnt == sequence_size:\n",
        "                clips.append(clip)\n",
        "                cnt = 0\n",
        "    return clips\n",
        "\n",
        "def get_clips_list(seq_size):\n",
        "    \"\"\"\n",
        "    seq_size :int \n",
        "        The sequence size of individual clip\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of training sequences of shape (NUMBER_OF_SEQUENCES,SINGLE_SEQUENCE_SIZE,FRAME_WIDTH,FRAME_HEIGHT,1)\n",
        "    \"\"\"\n",
        "    clips = []\n",
        "    # loop over the training folders (Train000,Train001,..)\n",
        "    for f in sorted(listdir(conf.DATASET_PATH)):\n",
        "        directory_path = join(conf.DATASET_PATH, f)\n",
        "        if isdir(directory_path):\n",
        "            all_frames = []\n",
        "            # loop over all the images in the folder (0.tif,1.tif,..,199.tif)\n",
        "            for c in sorted(listdir(directory_path)):\n",
        "                img_path = join(directory_path, c)\n",
        "                if str(img_path)[-3:] == \"tif\":\n",
        "                    img = Image.open(img_path).resize((256, 256))\n",
        "\n",
        "                    img = np.array(img, dtype=np.float32) / 256.0\n",
        "                    all_frames.append(img)\n",
        "            # get the 32-frames sequences from the list of images after applying data augmentation\n",
        "            for stride in range(1, 3):\n",
        "                clips.extend(get_clips_by_stride(stride=stride, frames_list=all_frames, sequence_size=seq_size))\n",
        "    return clips\n",
        "\n",
        "\n",
        "def get_single_test(single_test_path, sz):\n",
        "    test = np.zeros(shape=(sz, conf.dim2, conf.dim3, conf.dim4))\n",
        "    cnt = 0\n",
        "    for f in sorted(listdir(single_test_path)):\n",
        "        if str(join(single_test_path, f))[-3:] == \"tif\":\n",
        "            img = Image.open(join(single_test_path, f)).resize((conf.dim2, conf.dim3))\n",
        "            #cv2_imshow(np.array(img,dtype=np.float32))\n",
        "            #cv2.waitKey(0)\n",
        "            img = np.array(img, dtype=np.float32) / 256\n",
        "            test[cnt, :, :, 0] = img\n",
        "            cnt = cnt + 1\n",
        "    return test\n",
        "\n",
        "def evaluate(test_case_dir, model, sz, gen_only):\n",
        "\n",
        "    test = get_single_test(join(conf.TEST_DIR,test_case_dir), sz)\n",
        "    print(\"Test case loaded\")\n",
        "    sz = test.shape[0] - conf.dim1\n",
        "    sequences = np.zeros((sz, conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "    # apply the sliding window technique to get the sequences\n",
        "    for i in range(0, sz):\n",
        "        clip = np.zeros((conf.dim1, conf.dim2, conf.dim3, conf.dim4))\n",
        "        for j in range(0, conf.dim1):\n",
        "            clip[j] = test[i + j, :, :, :]\n",
        "        sequences[i] = clip\n",
        "\n",
        "    # get the reconstruction cost of all the sequences\n",
        "    reconstructed_sequences, sr = model.predict(sequences,batch_size=conf.BATCH_SIZE)\n",
        "    \n",
        "    if gen_only == 1:\n",
        "\n",
        "        for i in range(0, sz):\n",
        "            cv2_imshow(np.reshape(reconstructed_sequences[i][0],(256, 256))*256)\n",
        "            cv2.waitKey()\n",
        "\n",
        "        #reconstruction_shape = (sz,10, 256, 256, 1)\n",
        "        #sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sequences[i],reconstructed_sequences[i])) for i in range(0,sz)])\n",
        "        #sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)\n",
        "        #sr = 1.0 - sa\n",
        "    #print(sr.shape())\n",
        "\n",
        "    plt.plot(sr)\n",
        "    plt.ylabel('regularity score Sr(t)')\n",
        "    plt.xlabel('frame t')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCP4CpApLacO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_generator():\n",
        "    #if reload_model == True and os.path.isfile(conf.GEN_MODEL_PATH):\n",
        "    #    model=load_model(conf.GEN_MODEL_PATH,custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #    return model, True\n",
        "    print(\"Loading generator model\")\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(16, (11, 11), strides=(4,4), activation \"relu\", padding=\"same\"), batch_input_shape=(None, conf.dim1, conf.dim2, conf.dim3, conf.dim4)))   \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Conv2D(32, (5, 5), strides=(2,2), activation \"relu\", padding=\"same\")))\n",
        "    model.add(BatchNormalization())\n",
        "    #seq.add(Conv3D(16, (3, 3, 3), activation=\"relu\", strides=(1, 2, 2), padding=\"same\"))\n",
        "    #eq.add(BatchNormalization())\n",
        "    # # # # #\n",
        "    model.add(ConvLSTM2D(64,(3, 3), padding=\"same\", return_sequences=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ConvLSTM2D(16, (3, 3), padding=\"same\", return_sequences=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True))\n",
        "    model.add(BatchNormalization())\n",
        "    # # # # #\n",
        "    #seq.add(Conv3DTranspose(16, (3, 3, 3), activation=\"relu\", strides=(1, 2, 2), padding=\"same\"))\n",
        "    seq.add(TimeDistributed(Conv2DTranspose(32, (5, 5), strides=(2,2), activation \"relu\", padding=\"same\")))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Conv2DTranspose(16, (11, 11), strides=(4,4), activation \"relu\", padding=\"same\")))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Conv2D(1, (11, 11), activation=\"sigmoid\", padding=\"same\")))\n",
        "    #optimizer = tf.keras.optimizers.RMSprop(lr=0.002, clipvalue=1.0, decay=1e-8)\n",
        "    #seq.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "    model.summary(line_length=150)\n",
        "    return seq\n",
        "\n",
        "\n",
        "def get_discriminator():\n",
        "\n",
        "    #if reload_model == True and os.path.isfile(conf.DIS_MODEL_PATH):\n",
        "    #    model=load_model(conf.DIS_MODEL_PATH,custom_objects={'LayerNormalization': LayerNormalization})\n",
        "    #    return model, True\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1st layer group\n",
        "    model.add(Conv3D(16, (3, 3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1),name=\"conv1\", \n",
        "                     batch_input_shape=(None, conf.dim1, conf.dim2, conf.dim3, conf.dim4),\n",
        "                     strides=(1, 1, 1), padding=\"valid\"))  \n",
        "    #model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    # 2nd layer group  \n",
        "    model.add(Conv3D(32, (3, 3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1),name=\"conv2\", \n",
        "                     strides=(1, 1, 1), padding=\"valid\"))\n",
        "    #model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "   \n",
        "    model.add(Conv3D(64, (5, 5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.1),name=\"conv4\", \n",
        "                     strides=(1, 1, 1), padding=\"valid\"))   \n",
        "    m#odel.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "   \n",
        "    model.summary(line_length=150)\n",
        "    return model\n",
        "\n",
        "def build_model(model, image_dims):\n",
        "    input_img = Input(shape=image_dims)\n",
        "    output = model(input_img)\n",
        "    return Model(input_img,output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://medium.com/analytics-vidhya/implementing-a-gan-in-keras-d6c36bc6ab5f   \n",
        "#https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network/\n",
        "#https://arxiv.org/pdf/1802.09088.pdf\n",
        "get_generator()\n",
        "get_discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H9pUAm46jls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelContainer:\n",
        "  def __init__(self, y0, y1, y2):\n",
        "     self.generator = y0\n",
        "     self.discriminator = y1\n",
        "     self.gan = y2\n",
        "\n",
        "def compile_gan(generator, discriminator):\n",
        "    image_dims = [conf.dim1, conf.dim2, conf.dim3, conf.dim4]\n",
        "    optimizer = tf.keras.optimizers.Adadelta(lr=0.001)#RMSprop(lr=0.002, clipvalue=1.0, decay=1e-8)\n",
        "\n",
        "    built_dis =  build_model(discriminator, image_dims)\n",
        "\n",
        "    built_dis.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "\n",
        "    built_gen = build_model(generator, image_dims)\n",
        "    img = Input(shape=image_dims)\n",
        "\n",
        "    reconstructed_img = built_gen(img)\n",
        "\n",
        "    built_dis.trainable = False\n",
        "    validity = built_dis(reconstructed_img)\n",
        "\n",
        "    gan_model = Model(img, [reconstructed_img, validity])\n",
        "    gan_model.compile(loss=['mse', 'binary_crossentropy'],\n",
        "    loss_weights=[conf.r_alpha, 1],\n",
        "    optimizer=optimizer)\n",
        "\n",
        "    return ModelContainer(built_gen, built_dis, gan_model)\n",
        "    \n",
        "def train_step(models, batch_clips):\n",
        "    batch_noise_clips = np.multiply(batch_clips*256, np.random.normal(0, 1, size=(conf.BATCH_SIZE, conf.dim1, conf.dim2, conf.dim3, conf.dim4)))/256\n",
        "    batch_fake_clips = models.generator.predict_on_batch(batch_noise_clips)\n",
        "\n",
        "    d_loss_real = models.discriminator.train_on_batch(batch_clips, np.ones(shape=(conf.BATCH_SIZE,1)))\n",
        "    d_loss_fake = models.discriminator.train_on_batch(batch_fake_clips, np.zeros(shape=(conf.BATCH_SIZE,1)))\n",
        "    print(f'\\t\\t\\t\\t Discriminator Loss_Real: {d_loss_real} \\t\\t Loss_Fake: {d_loss_fake}\\n')\n",
        "    models.gan.train_on_batch(batch_noise_clips, [batch_clips, np.ones(shape=(conf.BATCH_SIZE,1))])\n",
        "    g_loss  = models.gan.train_on_batch(batch_noise_clips, [batch_clips, np.ones(shape=(conf.BATCH_SIZE,1))])\n",
        "    return  g_loss\n",
        "\n",
        "def train():\n",
        "    discriminator = get_discriminator()\n",
        "    generator = get_generator()\n",
        "    models = compile_gan(generator, discriminator)\n",
        "    models.gan.summary()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(np.array(get_clips_list(conf.dim1))).batch(conf.BATCH_SIZE)\n",
        "    \n",
        "    for epoch in range(conf.EPOCHS):\n",
        "        for batch in train_dataset:\n",
        "            \n",
        "            [total_weighted_loss, reconstruction_loss, fooling_loss] = train_step(models, batch)\n",
        "            print(f'Epoch: {epoch} \\t Discriminator Loss: {fooling_loss} \\t\\t Generator Loss: {reconstruction_loss} \\t\\t Total Loss: {total_weighted_loss}')\n",
        "            \n",
        "            models.gan.reset_states()\n",
        "        models.gan.save(\"training_gan/gan.hdf5\", save_format='h5')\n",
        "    \n",
        "    models.generator.save(conf.GEN_MODEL_PATH,save_format='h5')\n",
        "    models.discriminator.save(conf.DIS_MODEL_PATH,save_format='h5')\n",
        "    models.gan.save(conf.GAN_MODEL_PATH, save_format='h5')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC7JERZTIi3M",
        "colab_type": "code",
        "outputId": "228bc7eb-47c4-464d-ce7d-dd2e98cdc8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!rm training_gan -rf\n",
        "!mkdir training_gan\n",
        "if os.path.isfile(conf.GAN_MODEL_PATH):\n",
        "    model=load_model(conf.GAN_MODEL_PATH)\n",
        "    #,custom_objects={'LayerNormalization': LayerNormalization})\n",
        "else :\n",
        "    model = train()\n",
        "\n",
        "evaluate(\"Test002\", model, 200, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\t\t Discriminator Loss_Real: 0.6950267553329468 \t\t Loss_Fake: 0.7080869674682617\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6803192496299744 \t\t Generator Loss: 0.18686112761497498 \t\t Total Loss: 0.6803192496299744\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5792670249938965 \t\t Loss_Fake: 0.7041330933570862\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6779628992080688 \t\t Generator Loss: 0.18688052892684937 \t\t Total Loss: 0.6779628992080688\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5782437324523926 \t\t Loss_Fake: 0.7147606611251831\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6816049814224243 \t\t Generator Loss: 0.18679973483085632 \t\t Total Loss: 0.6816049814224243\n",
            "\t\t\t\t Discriminator Loss_Real: 0.4709138572216034 \t\t Loss_Fake: 0.7121996283531189\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6886969208717346 \t\t Generator Loss: 0.18672484159469604 \t\t Total Loss: 0.6886969208717346\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6073294878005981 \t\t Loss_Fake: 0.7089038491249084\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6735583543777466 \t\t Generator Loss: 0.18684139847755432 \t\t Total Loss: 0.6735583543777466\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5662646293640137 \t\t Loss_Fake: 0.6899969577789307\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6957817077636719 \t\t Generator Loss: 0.18720777332782745 \t\t Total Loss: 0.6957817077636719\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6952285766601562 \t\t Loss_Fake: 0.7050032615661621\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6825724840164185 \t\t Generator Loss: 0.18715986609458923 \t\t Total Loss: 0.6825724840164185\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6911842823028564 \t\t Loss_Fake: 0.6919456720352173\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6972002387046814 \t\t Generator Loss: 0.18724070489406586 \t\t Total Loss: 0.6972002387046814\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46242332458496094 \t\t Loss_Fake: 0.7027641534805298\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6737474203109741 \t\t Generator Loss: 0.1870836317539215 \t\t Total Loss: 0.6737474203109741\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5787911415100098 \t\t Loss_Fake: 0.7016658186912537\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6710494756698608 \t\t Generator Loss: 0.18718257546424866 \t\t Total Loss: 0.6710494756698608\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5793099403381348 \t\t Loss_Fake: 0.6953825950622559\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6947332620620728 \t\t Generator Loss: 0.18724948167800903 \t\t Total Loss: 0.6947332620620728\n",
            "\t\t\t\t Discriminator Loss_Real: 0.7196993231773376 \t\t Loss_Fake: 0.713687539100647\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6928809881210327 \t\t Generator Loss: 0.1872527152299881 \t\t Total Loss: 0.6928809881210327\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6743165254592896 \t\t Loss_Fake: 0.7013177275657654\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6750026345252991 \t\t Generator Loss: 0.18719661235809326 \t\t Total Loss: 0.6750026345252991\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6943650841712952 \t\t Loss_Fake: 0.6872623562812805\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6792656779289246 \t\t Generator Loss: 0.18720988929271698 \t\t Total Loss: 0.6792656779289246\n",
            "\t\t\t\t Discriminator Loss_Real: 0.47916582226753235 \t\t Loss_Fake: 0.7114204168319702\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6759000420570374 \t\t Generator Loss: 0.18718066811561584 \t\t Total Loss: 0.6759000420570374\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6936496496200562 \t\t Loss_Fake: 0.7160848379135132\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6840230226516724 \t\t Generator Loss: 0.1726236641407013 \t\t Total Loss: 0.6840230226516724\n",
            "\t\t\t\t Discriminator Loss_Real: 0.47392022609710693 \t\t Loss_Fake: 0.7003735303878784\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6835612058639526 \t\t Generator Loss: 0.1726779192686081 \t\t Total Loss: 0.6835612058639526\n",
            "\t\t\t\t Discriminator Loss_Real: 0.4728277921676636 \t\t Loss_Fake: 0.7004590034484863\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.676943302154541 \t\t Generator Loss: 0.1726512461900711 \t\t Total Loss: 0.676943302154541\n",
            "\t\t\t\t Discriminator Loss_Real: 0.579616129398346 \t\t Loss_Fake: 0.7129023671150208\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6867653131484985 \t\t Generator Loss: 0.17262814939022064 \t\t Total Loss: 0.6867653131484985\n",
            "\t\t\t\t Discriminator Loss_Real: 0.49461629986763 \t\t Loss_Fake: 0.7006345987319946\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.684958815574646 \t\t Generator Loss: 0.1726488322019577 \t\t Total Loss: 0.684958815574646\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5785500407218933 \t\t Loss_Fake: 0.6918718814849854\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6827675104141235 \t\t Generator Loss: 0.17268157005310059 \t\t Total Loss: 0.6827675104141235\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46387410163879395 \t\t Loss_Fake: 0.7142635583877563\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6924505233764648 \t\t Generator Loss: 0.17274019122123718 \t\t Total Loss: 0.6924505233764648\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5803838968276978 \t\t Loss_Fake: 0.6980890035629272\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6768540143966675 \t\t Generator Loss: 0.17266571521759033 \t\t Total Loss: 0.6768540143966675\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5806764960289001 \t\t Loss_Fake: 0.7026516199111938\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6791244149208069 \t\t Generator Loss: 0.17265848815441132 \t\t Total Loss: 0.6791244149208069\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46259286999702454 \t\t Loss_Fake: 0.6854739189147949\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6753408908843994 \t\t Generator Loss: 0.1727093607187271 \t\t Total Loss: 0.6753408908843994\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5381460189819336 \t\t Loss_Fake: 0.6949223279953003\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6839665174484253 \t\t Generator Loss: 0.17277474701404572 \t\t Total Loss: 0.6839665174484253\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5782418847084045 \t\t Loss_Fake: 0.6965359449386597\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6812242269515991 \t\t Generator Loss: 0.1727871149778366 \t\t Total Loss: 0.6812242269515991\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5814500451087952 \t\t Loss_Fake: 0.6934189796447754\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6843746900558472 \t\t Generator Loss: 0.17273125052452087 \t\t Total Loss: 0.6843746900558472\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5753002166748047 \t\t Loss_Fake: 0.7097991704940796\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6727870106697083 \t\t Generator Loss: 0.17279911041259766 \t\t Total Loss: 0.6727870106697083\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46239224076271057 \t\t Loss_Fake: 0.7064496278762817\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6902962327003479 \t\t Generator Loss: 0.17273202538490295 \t\t Total Loss: 0.6902962327003479\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6240776777267456 \t\t Loss_Fake: 0.6959476470947266\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6804736852645874 \t\t Generator Loss: 0.17269687354564667 \t\t Total Loss: 0.6804736852645874\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5806282758712769 \t\t Loss_Fake: 0.7277445793151855\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6775755286216736 \t\t Generator Loss: 0.17270345985889435 \t\t Total Loss: 0.6775755286216736\n",
            "\t\t\t\t Discriminator Loss_Real: 0.7046197056770325 \t\t Loss_Fake: 0.7111589908599854\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6888265013694763 \t\t Generator Loss: 0.17276790738105774 \t\t Total Loss: 0.6888265013694763\n",
            "\t\t\t\t Discriminator Loss_Real: 0.4729425609111786 \t\t Loss_Fake: 0.7035598754882812\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6981689929962158 \t\t Generator Loss: 0.17275431752204895 \t\t Total Loss: 0.6981689929962158\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46231982111930847 \t\t Loss_Fake: 0.707861602306366\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6943455934524536 \t\t Generator Loss: 0.17271079123020172 \t\t Total Loss: 0.6943455934524536\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5913323163986206 \t\t Loss_Fake: 0.6945128440856934\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6798540353775024 \t\t Generator Loss: 0.17000749707221985 \t\t Total Loss: 0.6798540353775024\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5763428807258606 \t\t Loss_Fake: 0.7028114795684814\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6877830028533936 \t\t Generator Loss: 0.170099675655365 \t\t Total Loss: 0.6877830028533936\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5789111256599426 \t\t Loss_Fake: 0.7118976712226868\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6923965215682983 \t\t Generator Loss: 0.16999021172523499 \t\t Total Loss: 0.6923965215682983\n",
            "\t\t\t\t Discriminator Loss_Real: 0.47142696380615234 \t\t Loss_Fake: 0.6900638341903687\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6803367137908936 \t\t Generator Loss: 0.16999301314353943 \t\t Total Loss: 0.6803367137908936\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5738795399665833 \t\t Loss_Fake: 0.7086882591247559\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6760661005973816 \t\t Generator Loss: 0.16996698081493378 \t\t Total Loss: 0.6760661005973816\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6951993703842163 \t\t Loss_Fake: 0.7101938724517822\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6712832450866699 \t\t Generator Loss: 0.17012760043144226 \t\t Total Loss: 0.6712832450866699\n",
            "\t\t\t\t Discriminator Loss_Real: 0.578533411026001 \t\t Loss_Fake: 0.7119115591049194\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6931345462799072 \t\t Generator Loss: 0.17013424634933472 \t\t Total Loss: 0.6931345462799072\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5777877569198608 \t\t Loss_Fake: 0.7106205224990845\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6708036661148071 \t\t Generator Loss: 0.170132577419281 \t\t Total Loss: 0.6708036661148071\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5800988078117371 \t\t Loss_Fake: 0.7020812034606934\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6826033592224121 \t\t Generator Loss: 0.17010854184627533 \t\t Total Loss: 0.6826033592224121\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5785198211669922 \t\t Loss_Fake: 0.697974443435669\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6750574111938477 \t\t Generator Loss: 0.1700625717639923 \t\t Total Loss: 0.6750574111938477\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6791702508926392 \t\t Loss_Fake: 0.706916093826294\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6835291981697083 \t\t Generator Loss: 0.17011766135692596 \t\t Total Loss: 0.6835291981697083\n",
            "\t\t\t\t Discriminator Loss_Real: 0.46269935369491577 \t\t Loss_Fake: 0.701879620552063\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6772240996360779 \t\t Generator Loss: 0.17002981901168823 \t\t Total Loss: 0.6772240996360779\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5898368954658508 \t\t Loss_Fake: 0.7010529041290283\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6723276376724243 \t\t Generator Loss: 0.17013657093048096 \t\t Total Loss: 0.6723276376724243\n",
            "\t\t\t\t Discriminator Loss_Real: 0.5885980129241943 \t\t Loss_Fake: 0.7024455666542053\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6910269260406494 \t\t Generator Loss: 0.17007634043693542 \t\t Total Loss: 0.6910269260406494\n",
            "\t\t\t\t Discriminator Loss_Real: 0.61809903383255 \t\t Loss_Fake: 0.7064148783683777\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6920249462127686 \t\t Generator Loss: 0.17018277943134308 \t\t Total Loss: 0.6920249462127686\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6955966949462891 \t\t Loss_Fake: 0.7137809991836548\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6854948401451111 \t\t Generator Loss: 0.1700495183467865 \t\t Total Loss: 0.6854948401451111\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6941267251968384 \t\t Loss_Fake: 0.7042434215545654\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6744886040687561 \t\t Generator Loss: 0.17013171315193176 \t\t Total Loss: 0.6744886040687561\n",
            "\t\t\t\t Discriminator Loss_Real: 0.6950492858886719 \t\t Loss_Fake: 0.7123389840126038\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6888912916183472 \t\t Generator Loss: 0.17002640664577484 \t\t Total Loss: 0.6888912916183472\n",
            "\t\t\t\t Discriminator Loss_Real: 0.47145724296569824 \t\t Loss_Fake: 0.7036484479904175\n",
            "\n",
            "Epoch: 13 \t Discriminator Loss: 0.6676899194717407 \t\t Generator Loss: 0.17012353241443634 \t\t Total Loss: 0.6676899194717407\n",
            "\t\t\t\t Discriminator Loss_Real: 0.7198854684829712 \t\t Loss_Fake: 0.7089260220527649\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}